{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 14:04:20.658968: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 14:04:20.798230: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-19 14:04:21.405442: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-19 14:04:21.405541: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-19 14:04:21.405548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "from settings import *\n",
    "from src.data import generate\n",
    "\n",
    "instruments = ['Guitar']\n",
    "instruments_aug = ['Accordion', 'Violin', 'Piano']\n",
    "\n",
    "# from setup_logging import setup_logging\n",
    "# setup_logging()\n",
    "\n",
    "#generate.my_run(instruments)\n",
    "datasets_raw = [pd.read_pickle(os.path.join(METADATA_DIR_PROCESSED, f'data_{instrument.lower()}.pkl')) for instrument in instruments]\n",
    "datasets_augmented = [pd.read_pickle(os.path.join(METADATA_DIR_AUGMENTED_RAW, f'data_{instrument.lower()}.pkl')) for instrument in instruments_aug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 60), (128, 65), (128, 73), (128, 76), (128, 61), (128, 72), (128, 69), (128, 58), (128, 51), (128, 65), (128, 54), (128, 72), (128, 81), (128, 61), (128, 65), (128, 35), (128, 65), (128, 81), (128, 21), (128, 73), (128, 67), (128, 27), (128, 81), (128, 74), (128, 78), (128, 47), (128, 81), (128, 81), (128, 81), (128, 81), (128, 55), (128, 46), (128, 66), (128, 73), (128, 53), (128, 78), (128, 68), (128, 60), (128, 67), (128, 59), (128, 81), (128, 79), (128, 64), (128, 75), (128, 81), (128, 81), (128, 81), (128, 39), (128, 70), (128, 81), (128, 81), (128, 81), (128, 47), (128, 81), (128, 68), (128, 81), (128, 81), (128, 81), (128, 46), (128, 40), (128, 81), (128, 75), (128, 73), (128, 61), (128, 44), (128, 81), (128, 81), (128, 73), (128, 81), (128, 81), (128, 25), (128, 72), (128, 51), (128, 81), (128, 81), (128, 81), (128, 66), (128, 65), (128, 52), (128, 62), (128, 81), (128, 72), (128, 81), (128, 57), (128, 65), (128, 71), (128, 76), (128, 26), (128, 56), (128, 81), (128, 50), (128, 46), (128, 47), (128, 45), (128, 48), (128, 58), (128, 48), (128, 46), (128, 74), (128, 39), (128, 79), (128, 86), (128, 97), (128, 100), (128, 81), (128, 95), (128, 91), (128, 77), (128, 67), (128, 85), (128, 71), (128, 95), (128, 107), (128, 81), (128, 86), (128, 46), (128, 86), (128, 107), (128, 27), (128, 96), (128, 89), (128, 35), (128, 107), (128, 98), (128, 103), (128, 62), (128, 107), (128, 107), (128, 107), (128, 107), (128, 72), (128, 60), (128, 88), (128, 96), (128, 70), (128, 103), (128, 90), (128, 80), (128, 89), (128, 78), (128, 107), (128, 104), (128, 84), (128, 99), (128, 107), (128, 107), (128, 107), (128, 52), (128, 92), (128, 107), (128, 107), (128, 107), (128, 62), (128, 107), (128, 90), (128, 107), (128, 107), (128, 107), (128, 60), (128, 53), (128, 107), (128, 99), (128, 97), (128, 81), (128, 59), (128, 107), (128, 107), (128, 96), (128, 107), (128, 107), (128, 33), (128, 95), (128, 67), (128, 107), (128, 107), (128, 107), (128, 88), (128, 86), (128, 68), (128, 81), (128, 107), (128, 96), (128, 107), (128, 75), (128, 86), (128, 93), (128, 101), (128, 34), (128, 73), (128, 107), (128, 65), (128, 60), (128, 62), (128, 60), (128, 63), (128, 76), (128, 63), (128, 61), (128, 98), (128, 52), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42)]\n"
     ]
    }
   ],
   "source": [
    "print([x.shape for x in datasets_augmented[0]['spectrogram']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.preprocessing import get_max_shape\n",
    "max_spectrogram_size = max(map(lambda df: get_max_shape(df), datasets_raw+datasets_augmented))\n",
    "max_spectrogram_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.preprocessing import uniform_shape\n",
    "uniform = lambda df: uniform_shape(df, max_spectrogram_size)\n",
    "\n",
    "datasets_raw = list(map(uniform,datasets_raw))\n",
    "datasets_augmented = list(map(uniform,datasets_augmented))\n",
    "datasets_augmented = list(map(lambda df: df[['spectrogram','class_ID', 'class_name','augmentation']],datasets_augmented))\n",
    "datasets_augmented = list(map(lambda df: df.reset_index(drop=True), datasets_augmented))\n",
    "#datasets_augmented = list(map(lambda df: df[['spectrogram','class_ID', 'class_name','augmentation']],map(uniform,datasets_augmented)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 107)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.preprocessing import all_equal\n",
    "#equal_shape = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.processing - INFO - Start train test split with split ratio: 0.65\n",
      "src.processing - INFO - Number of training samples is 1300\n",
      "src.processing - INFO - Number of testing samples is 700\n",
      "src.processing - INFO - Train test split completed\n",
      "src.processing - INFO - Start train test split with split ratio: 0.65\n",
      "src.processing - INFO - Number of training samples is 585\n",
      "src.processing - INFO - Number of testing samples is 315\n",
      "src.processing - INFO - Train test split completed\n",
      "src.processing - INFO - Start train test split with split ratio: 0.65\n",
      "src.processing - INFO - Number of training samples is 585\n",
      "src.processing - INFO - Number of testing samples is 315\n",
      "src.processing - INFO - Train test split completed\n",
      "src.processing - INFO - Start train test split with split ratio: 0.65\n",
      "src.processing - INFO - Number of training samples is 585\n",
      "src.processing - INFO - Number of testing samples is 315\n",
      "src.processing - INFO - Train test split completed\n",
      "The most frequent shape is (128, 107)\n",
      "src.train - INFO - Number of train samples: 3055\n",
      "src.train - INFO - Number of test samples: 1645\n",
      "src.processing - INFO - Start feature target split\n",
      "src.processing - INFO - Feature target split completed\n",
      "src.processing - INFO - Start feature target split\n",
      "src.processing - INFO - Feature target split completed\n",
      "src.processing - INFO - Features reshaped for CNN Input\n",
      "src.processing - INFO - Features reshaped for CNN Input\n",
      "src.processing - INFO - Target one hot encoded\n",
      "src.processing - INFO - Target one hot encoded\n",
      "src.model - INFO - Initializing CNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 14:04:29.265330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22324 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:3b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.model - INFO - Input shape = (128, 107, 1)\n",
      "src.model - INFO - CNN Initialized\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 124, 103, 24)      624       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 51, 24)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " activation (Activation)     (None, 31, 51, 24)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 47, 48)        28848     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 23, 48)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6, 23, 48)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 2, 19, 48)         57648     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2, 19, 48)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1824)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1824)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                116800    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 204,570\n",
      "Trainable params: 204,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "src.train - INFO - None\n",
      "src.model - INFO - Start training model\n",
      "src.model - INFO - Tensorboard Logging Started\n",
      "src.model - INFO - Use the following command in the terminal to view the logs during training: tensorboard --logdir logs/training\n",
      "Epoch 1/67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 14:04:32.541253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8600\n",
      "2023-05-19 14:04:33.327180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - ETA: 0s - loss: 2.7696 - accuracy: 0.1260 - precision: 0.1079 - recall: 0.0111 - fmeasure: 0.0194src.model - INFO - {Epoch: 0} loss: 2.769570, accuracy: 0.126023, precision: 0.107874, recall: 0.011111, fmeasure: 0.019390, val_loss: 2.211191, val_accuracy: 0.196353, val_precision: 0.204819, val_recall: 0.011446, val_fmeasure: 0.021593\n",
      "153/153 [==============================] - 7s 17ms/step - loss: 2.7696 - accuracy: 0.1260 - precision: 0.1079 - recall: 0.0111 - fmeasure: 0.0194 - val_loss: 2.2112 - val_accuracy: 0.1964 - val_precision: 0.2048 - val_recall: 0.0114 - val_fmeasure: 0.0216\n",
      "Epoch 2/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 2.1675 - accuracy: 0.1944 - precision: 0.4615 - recall: 0.0454 - fmeasure: 0.0806src.model - INFO - {Epoch: 1} loss: 2.168765, accuracy: 0.194435, precision: 0.458715, recall: 0.045425, fmeasure: 0.080633, val_loss: 2.044526, val_accuracy: 0.275988, val_precision: 0.726908, val_recall: 0.059036, val_fmeasure: 0.107674\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 2.1688 - accuracy: 0.1944 - precision: 0.4587 - recall: 0.0454 - fmeasure: 0.0806 - val_loss: 2.0445 - val_accuracy: 0.2760 - val_precision: 0.7269 - val_recall: 0.0590 - val_fmeasure: 0.1077\n",
      "Epoch 3/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 2.0911 - accuracy: 0.2318 - precision: 0.6582 - recall: 0.0801 - fmeasure: 0.1389src.model - INFO - {Epoch: 2} loss: 2.088995, accuracy: 0.232733, precision: 0.662691, recall: 0.080828, fmeasure: 0.140128, val_loss: 1.953830, val_accuracy: 0.296657, val_precision: 0.868474, val_recall: 0.104819, val_fmeasure: 0.183185\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 2.0890 - accuracy: 0.2327 - precision: 0.6627 - recall: 0.0808 - fmeasure: 0.1401 - val_loss: 1.9538 - val_accuracy: 0.2967 - val_precision: 0.8685 - val_recall: 0.1048 - val_fmeasure: 0.1832\n",
      "Epoch 4/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 2.0211 - accuracy: 0.2570 - precision: 0.7333 - recall: 0.1089 - fmeasure: 0.1846src.model - INFO - {Epoch: 3} loss: 2.018928, accuracy: 0.258592, precision: 0.735745, recall: 0.110893, fmeasure: 0.187449, val_loss: 1.924165, val_accuracy: 0.305775, val_precision: 0.842771, val_recall: 0.136145, val_fmeasure: 0.229501\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 2.0189 - accuracy: 0.2586 - precision: 0.7357 - recall: 0.1109 - fmeasure: 0.1874 - val_loss: 1.9242 - val_accuracy: 0.3058 - val_precision: 0.8428 - val_recall: 0.1361 - val_fmeasure: 0.2295\n",
      "Epoch 5/67\n",
      "152/153 [============================>.] - ETA: 0s - loss: 1.9254 - accuracy: 0.2921 - precision: 0.8076 - recall: 0.1474 - fmeasure: 0.2423src.model - INFO - {Epoch: 4} loss: 1.925718, accuracy: 0.291980, precision: 0.808886, recall: 0.146841, fmeasure: 0.241511, val_loss: 1.813021, val_accuracy: 0.348328, val_precision: 0.855890, val_recall: 0.186145, val_fmeasure: 0.297508\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 1.9257 - accuracy: 0.2920 - precision: 0.8089 - recall: 0.1468 - fmeasure: 0.2415 - val_loss: 1.8130 - val_accuracy: 0.3483 - val_precision: 0.8559 - val_recall: 0.1861 - val_fmeasure: 0.2975\n",
      "Epoch 6/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 1.9261 - accuracy: 0.3096 - precision: 0.8146 - recall: 0.1583 - fmeasure: 0.2574src.model - INFO - {Epoch: 5} loss: 1.926268, accuracy: 0.308674, precision: 0.813757, recall: 0.157734, fmeasure: 0.256625, val_loss: 1.851313, val_accuracy: 0.365957, val_precision: 0.889458, val_recall: 0.151807, val_fmeasure: 0.252872\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.9263 - accuracy: 0.3087 - precision: 0.8138 - recall: 0.1577 - fmeasure: 0.2566 - val_loss: 1.8513 - val_accuracy: 0.3660 - val_precision: 0.8895 - val_recall: 0.1518 - val_fmeasure: 0.2529\n",
      "Epoch 7/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 1.8152 - accuracy: 0.3348 - precision: 0.8465 - recall: 0.1844 - fmeasure: 0.2953src.model - INFO - {Epoch: 6} loss: 1.825438, accuracy: 0.334206, precision: 0.845262, recall: 0.185076, fmeasure: 0.296103, val_loss: 1.700539, val_accuracy: 0.424316, val_precision: 0.907167, val_recall: 0.223494, val_fmeasure: 0.350709\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.8254 - accuracy: 0.3342 - precision: 0.8453 - recall: 0.1851 - fmeasure: 0.2961 - val_loss: 1.7005 - val_accuracy: 0.4243 - val_precision: 0.9072 - val_recall: 0.2235 - val_fmeasure: 0.3507\n",
      "Epoch 8/67\n",
      "150/153 [============================>.] - ETA: 0s - loss: 1.8201 - accuracy: 0.3447 - precision: 0.8362 - recall: 0.1960 - fmeasure: 0.3114src.model - INFO - {Epoch: 7} loss: 1.820150, accuracy: 0.344026, precision: 0.835102, recall: 0.196187, fmeasure: 0.311658, val_loss: 1.730748, val_accuracy: 0.416413, val_precision: 0.844673, val_recall: 0.239759, val_fmeasure: 0.366360\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.8201 - accuracy: 0.3440 - precision: 0.8351 - recall: 0.1962 - fmeasure: 0.3117 - val_loss: 1.7307 - val_accuracy: 0.4164 - val_precision: 0.8447 - val_recall: 0.2398 - val_fmeasure: 0.3664\n",
      "Epoch 9/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 1.7065 - accuracy: 0.3828 - precision: 0.8668 - recall: 0.2278 - fmeasure: 0.3519src.model - INFO - {Epoch: 8} loss: 1.705619, accuracy: 0.383306, precision: 0.865302, recall: 0.228214, fmeasure: 0.352290, val_loss: 1.570537, val_accuracy: 0.444377, val_precision: 0.903562, val_recall: 0.253614, val_fmeasure: 0.387558\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.7056 - accuracy: 0.3833 - precision: 0.8653 - recall: 0.2282 - fmeasure: 0.3523 - val_loss: 1.5705 - val_accuracy: 0.4444 - val_precision: 0.9036 - val_recall: 0.2536 - val_fmeasure: 0.3876\n",
      "Epoch 10/67\n",
      "150/153 [============================>.] - ETA: 0s - loss: 1.7458 - accuracy: 0.3783 - precision: 0.8171 - recall: 0.2193 - fmeasure: 0.3379src.model - INFO - {Epoch: 9} loss: 1.739540, accuracy: 0.381015, precision: 0.819403, recall: 0.221242, fmeasure: 0.340157, val_loss: 1.576084, val_accuracy: 0.462006, val_precision: 0.887736, val_recall: 0.275301, val_fmeasure: 0.413253\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 1.7395 - accuracy: 0.3810 - precision: 0.8194 - recall: 0.2212 - fmeasure: 0.3402 - val_loss: 1.5761 - val_accuracy: 0.4620 - val_precision: 0.8877 - val_recall: 0.2753 - val_fmeasure: 0.4133\n",
      "Epoch 11/67\n",
      "150/153 [============================>.] - ETA: 0s - loss: 1.6318 - accuracy: 0.4240 - precision: 0.8300 - recall: 0.2530 - fmeasure: 0.3789src.model - INFO - {Epoch: 10} loss: 1.628782, accuracy: 0.423895, precision: 0.827439, recall: 0.253050, fmeasure: 0.378800, val_loss: 1.472882, val_accuracy: 0.500304, val_precision: 0.883401, val_recall: 0.318675, val_fmeasure: 0.459311\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.6288 - accuracy: 0.4239 - precision: 0.8274 - recall: 0.2531 - fmeasure: 0.3788 - val_loss: 1.4729 - val_accuracy: 0.5003 - val_precision: 0.8834 - val_recall: 0.3187 - val_fmeasure: 0.4593\n",
      "Epoch 12/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 1.6459 - accuracy: 0.4318 - precision: 0.8247 - recall: 0.2609 - fmeasure: 0.3890src.model - INFO - {Epoch: 11} loss: 1.640003, accuracy: 0.433715, precision: 0.826499, recall: 0.262309, fmeasure: 0.390232, val_loss: 1.396308, val_accuracy: 0.542249, val_precision: 0.926647, val_recall: 0.326506, val_fmeasure: 0.472761\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.6400 - accuracy: 0.4337 - precision: 0.8265 - recall: 0.2623 - fmeasure: 0.3902 - val_loss: 1.3963 - val_accuracy: 0.5422 - val_precision: 0.9266 - val_recall: 0.3265 - val_fmeasure: 0.4728\n",
      "Epoch 13/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 1.5623 - accuracy: 0.4500 - precision: 0.8474 - recall: 0.2947 - fmeasure: 0.4294src.model - INFO - {Epoch: 12} loss: 1.560888, accuracy: 0.451064, precision: 0.846416, recall: 0.293464, fmeasure: 0.427931, val_loss: 1.354653, val_accuracy: 0.556839, val_precision: 0.921985, val_recall: 0.342771, val_fmeasure: 0.490273\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.5609 - accuracy: 0.4511 - precision: 0.8464 - recall: 0.2935 - fmeasure: 0.4279 - val_loss: 1.3547 - val_accuracy: 0.5568 - val_precision: 0.9220 - val_recall: 0.3428 - val_fmeasure: 0.4903\n",
      "Epoch 14/67\n",
      "150/153 [============================>.] - ETA: 0s - loss: 1.4117 - accuracy: 0.5143 - precision: 0.8714 - recall: 0.3407 - fmeasure: 0.4819src.model - INFO - {Epoch: 13} loss: 1.415226, accuracy: 0.512930, precision: 0.869086, recall: 0.338235, fmeasure: 0.479049, val_loss: 1.314826, val_accuracy: 0.600000, val_precision: 0.929948, val_recall: 0.380723, val_fmeasure: 0.532057\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 1.4152 - accuracy: 0.5129 - precision: 0.8691 - recall: 0.3382 - fmeasure: 0.4790 - val_loss: 1.3148 - val_accuracy: 0.6000 - val_precision: 0.9299 - val_recall: 0.3807 - val_fmeasure: 0.5321\n",
      "Epoch 15/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 1.3771 - accuracy: 0.5136 - precision: 0.8384 - recall: 0.3646 - fmeasure: 0.5013src.model - INFO - {Epoch: 14} loss: 1.384240, accuracy: 0.509984, precision: 0.837290, recall: 0.362309, fmeasure: 0.498686, val_loss: 1.181628, val_accuracy: 0.617021, val_precision: 0.950909, val_recall: 0.408434, val_fmeasure: 0.562952\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.3842 - accuracy: 0.5100 - precision: 0.8373 - recall: 0.3623 - fmeasure: 0.4987 - val_loss: 1.1816 - val_accuracy: 0.6170 - val_precision: 0.9509 - val_recall: 0.4084 - val_fmeasure: 0.5630\n",
      "Epoch 16/67\n",
      "150/153 [============================>.] - ETA: 0s - loss: 1.2779 - accuracy: 0.5543 - precision: 0.8606 - recall: 0.4110 - fmeasure: 0.5481src.model - INFO - {Epoch: 15} loss: 1.277352, accuracy: 0.553846, precision: 0.860579, recall: 0.410893, fmeasure: 0.548141, val_loss: 1.147662, val_accuracy: 0.644377, val_precision: 0.939911, val_recall: 0.436747, val_fmeasure: 0.588549\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.2774 - accuracy: 0.5538 - precision: 0.8606 - recall: 0.4109 - fmeasure: 0.5481 - val_loss: 1.1477 - val_accuracy: 0.6444 - val_precision: 0.9399 - val_recall: 0.4367 - val_fmeasure: 0.5885\n",
      "Epoch 17/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 1.2978 - accuracy: 0.5629 - precision: 0.8556 - recall: 0.4189 - fmeasure: 0.5562src.model - INFO - {Epoch: 16} loss: 1.294842, accuracy: 0.563666, precision: 0.855838, recall: 0.418736, fmeasure: 0.556210, val_loss: 1.100975, val_accuracy: 0.659574, val_precision: 0.933342, val_recall: 0.481928, val_fmeasure: 0.629019\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.2948 - accuracy: 0.5637 - precision: 0.8558 - recall: 0.4187 - fmeasure: 0.5562 - val_loss: 1.1010 - val_accuracy: 0.6596 - val_precision: 0.9333 - val_recall: 0.4819 - val_fmeasure: 0.6290\n",
      "Epoch 18/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 1.1037 - accuracy: 0.6189 - precision: 0.8768 - recall: 0.4811 - fmeasure: 0.6150src.model - INFO - {Epoch: 17} loss: 1.102028, accuracy: 0.620295, precision: 0.877887, recall: 0.482789, fmeasure: 0.616640, val_loss: 0.973273, val_accuracy: 0.702736, val_precision: 0.954453, val_recall: 0.575904, val_fmeasure: 0.713996\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.1020 - accuracy: 0.6203 - precision: 0.8779 - recall: 0.4828 - fmeasure: 0.6166 - val_loss: 0.9733 - val_accuracy: 0.7027 - val_precision: 0.9545 - val_recall: 0.5759 - val_fmeasure: 0.7140\n",
      "Epoch 19/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.9512 - accuracy: 0.6699 - precision: 0.9084 - recall: 0.5464 - fmeasure: 0.6767src.model - INFO - {Epoch: 18} loss: 0.949302, accuracy: 0.669722, precision: 0.908366, recall: 0.546950, fmeasure: 0.677253, val_loss: 0.897373, val_accuracy: 0.711854, val_precision: 0.945091, val_recall: 0.588554, val_fmeasure: 0.720894\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.9493 - accuracy: 0.6697 - precision: 0.9084 - recall: 0.5469 - fmeasure: 0.6773 - val_loss: 0.8974 - val_accuracy: 0.7119 - val_precision: 0.9451 - val_recall: 0.5886 - val_fmeasure: 0.7209\n",
      "Epoch 20/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.9401 - accuracy: 0.6825 - precision: 0.8893 - recall: 0.5596 - fmeasure: 0.6825src.model - INFO - {Epoch: 19} loss: 0.939609, accuracy: 0.683142, precision: 0.888639, recall: 0.559368, fmeasure: 0.682125, val_loss: 0.839402, val_accuracy: 0.750152, val_precision: 0.931369, val_recall: 0.611446, val_fmeasure: 0.734400\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.9396 - accuracy: 0.6831 - precision: 0.8886 - recall: 0.5594 - fmeasure: 0.6821 - val_loss: 0.8394 - val_accuracy: 0.7502 - val_precision: 0.9314 - val_recall: 0.6114 - val_fmeasure: 0.7344\n",
      "Epoch 21/67\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.9554 - accuracy: 0.7010 - precision: 0.8905 - recall: 0.5772 - fmeasure: 0.6960src.model - INFO - {Epoch: 20} loss: 0.950674, accuracy: 0.701146, precision: 0.890417, recall: 0.579412, fmeasure: 0.697660, val_loss: 0.852176, val_accuracy: 0.745289, val_precision: 0.926317, val_recall: 0.584940, val_fmeasure: 0.712620\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.9507 - accuracy: 0.7011 - precision: 0.8904 - recall: 0.5794 - fmeasure: 0.6977 - val_loss: 0.8522 - val_accuracy: 0.7453 - val_precision: 0.9263 - val_recall: 0.5849 - val_fmeasure: 0.7126\n",
      "Epoch 22/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.9601 - accuracy: 0.6904 - precision: 0.8694 - recall: 0.5570 - fmeasure: 0.6742src.model - INFO - {Epoch: 21} loss: 0.955323, accuracy: 0.692308, precision: 0.870639, recall: 0.558279, fmeasure: 0.675604, val_loss: 0.800125, val_accuracy: 0.770213, val_precision: 0.937174, val_recall: 0.616867, val_fmeasure: 0.739648\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.9553 - accuracy: 0.6923 - precision: 0.8706 - recall: 0.5583 - fmeasure: 0.6756 - val_loss: 0.8001 - val_accuracy: 0.7702 - val_precision: 0.9372 - val_recall: 0.6169 - val_fmeasure: 0.7396\n",
      "Epoch 23/67\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.8335 - accuracy: 0.7175 - precision: 0.8768 - recall: 0.5973 - fmeasure: 0.7064src.model - INFO - {Epoch: 22} loss: 0.833547, accuracy: 0.717512, precision: 0.876780, recall: 0.597277, fmeasure: 0.706440, val_loss: 0.833985, val_accuracy: 0.778116, val_precision: 0.899573, val_recall: 0.680723, val_fmeasure: 0.772001\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.8335 - accuracy: 0.7175 - precision: 0.8768 - recall: 0.5973 - fmeasure: 0.7064 - val_loss: 0.8340 - val_accuracy: 0.7781 - val_precision: 0.8996 - val_recall: 0.6807 - val_fmeasure: 0.7720\n",
      "Epoch 24/67\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.7158 - accuracy: 0.7544 - precision: 0.8994 - recall: 0.6534 - fmeasure: 0.7525src.model - INFO - {Epoch: 23} loss: 0.718380, accuracy: 0.752537, precision: 0.898229, recall: 0.652070, fmeasure: 0.751235, val_loss: 0.684420, val_accuracy: 0.813982, val_precision: 0.922892, val_recall: 0.704819, val_fmeasure: 0.796799\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.7184 - accuracy: 0.7525 - precision: 0.8982 - recall: 0.6521 - fmeasure: 0.7512 - val_loss: 0.6844 - val_accuracy: 0.8140 - val_precision: 0.9229 - val_recall: 0.7048 - val_fmeasure: 0.7968\n",
      "Epoch 25/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 1.2684 - accuracy: 0.7013 - precision: 0.8326 - recall: 0.5709 - fmeasure: 0.6687src.model - INFO - {Epoch: 24} loss: 1.269062, accuracy: 0.699509, precision: 0.831188, recall: 0.568192, fmeasure: 0.666337, val_loss: 1.033643, val_accuracy: 0.686930, val_precision: 0.858736, val_recall: 0.526506, val_fmeasure: 0.647444\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 1.2691 - accuracy: 0.6995 - precision: 0.8312 - recall: 0.5682 - fmeasure: 0.6663 - val_loss: 1.0336 - val_accuracy: 0.6869 - val_precision: 0.8587 - val_recall: 0.5265 - val_fmeasure: 0.6474\n",
      "Epoch 26/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.9114 - accuracy: 0.7040 - precision: 0.8535 - recall: 0.5692 - fmeasure: 0.6784src.model - INFO - {Epoch: 25} loss: 0.908561, accuracy: 0.704419, precision: 0.853892, recall: 0.570370, fmeasure: 0.679322, val_loss: 0.687752, val_accuracy: 0.811550, val_precision: 0.927308, val_recall: 0.674096, val_fmeasure: 0.777267\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.9086 - accuracy: 0.7044 - precision: 0.8539 - recall: 0.5704 - fmeasure: 0.6793 - val_loss: 0.6878 - val_accuracy: 0.8116 - val_precision: 0.9273 - val_recall: 0.6741 - val_fmeasure: 0.7773\n",
      "Epoch 27/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.7111 - accuracy: 0.7536 - precision: 0.8839 - recall: 0.6523 - fmeasure: 0.7471src.model - INFO - {Epoch: 26} loss: 0.709354, accuracy: 0.754173, precision: 0.884387, recall: 0.652396, fmeasure: 0.747372, val_loss: 0.610030, val_accuracy: 0.840122, val_precision: 0.915618, val_recall: 0.729518, val_fmeasure: 0.809547\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.7094 - accuracy: 0.7542 - precision: 0.8844 - recall: 0.6524 - fmeasure: 0.7474 - val_loss: 0.6100 - val_accuracy: 0.8401 - val_precision: 0.9156 - val_recall: 0.7295 - val_fmeasure: 0.8095\n",
      "Epoch 28/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.6243 - accuracy: 0.7974 - precision: 0.8980 - recall: 0.6904 - fmeasure: 0.7778src.model - INFO - {Epoch: 27} loss: 0.622918, accuracy: 0.797381, precision: 0.897130, recall: 0.690632, fmeasure: 0.777610, val_loss: 0.580879, val_accuracy: 0.851672, val_precision: 0.923164, val_recall: 0.752410, val_fmeasure: 0.827076\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.6229 - accuracy: 0.7974 - precision: 0.8971 - recall: 0.6906 - fmeasure: 0.7776 - val_loss: 0.5809 - val_accuracy: 0.8517 - val_precision: 0.9232 - val_recall: 0.7524 - val_fmeasure: 0.8271\n",
      "Epoch 29/67\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.8105 - precision: 0.9120 - recall: 0.7342 - fmeasure: 0.8113src.model - INFO - {Epoch: 28} loss: 0.538024, accuracy: 0.810475, precision: 0.912027, recall: 0.734205, fmeasure: 0.811273, val_loss: 0.560867, val_accuracy: 0.847416, val_precision: 0.928050, val_recall: 0.760843, val_fmeasure: 0.833983\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.5380 - accuracy: 0.8105 - precision: 0.9120 - recall: 0.7342 - fmeasure: 0.8113 - val_loss: 0.5609 - val_accuracy: 0.8474 - val_precision: 0.9280 - val_recall: 0.7608 - val_fmeasure: 0.8340\n",
      "Epoch 30/67\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.5133 - accuracy: 0.8223 - precision: 0.9067 - recall: 0.7510 - fmeasure: 0.8197src.model - INFO - {Epoch: 29} loss: 0.512206, accuracy: 0.822586, precision: 0.906848, recall: 0.751308, fmeasure: 0.819949, val_loss: 0.515311, val_accuracy: 0.865046, val_precision: 0.919832, val_recall: 0.778313, val_fmeasure: 0.841464\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.5122 - accuracy: 0.8226 - precision: 0.9068 - recall: 0.7513 - fmeasure: 0.8199 - val_loss: 0.5153 - val_accuracy: 0.8650 - val_precision: 0.9198 - val_recall: 0.7783 - val_fmeasure: 0.8415\n",
      "Epoch 31/67\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.5844 - accuracy: 0.8067 - precision: 0.8861 - recall: 0.7297 - fmeasure: 0.7978src.model - INFO - {Epoch: 30} loss: 0.583448, accuracy: 0.806874, precision: 0.886415, recall: 0.729739, fmeasure: 0.797989, val_loss: 0.605229, val_accuracy: 0.838298, val_precision: 0.908427, val_recall: 0.765060, val_fmeasure: 0.828932\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.5834 - accuracy: 0.8069 - precision: 0.8864 - recall: 0.7297 - fmeasure: 0.7980 - val_loss: 0.6052 - val_accuracy: 0.8383 - val_precision: 0.9084 - val_recall: 0.7651 - val_fmeasure: 0.8289\n",
      "Epoch 32/67\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.5070 - accuracy: 0.8237 - precision: 0.8987 - recall: 0.7520 - fmeasure: 0.8164src.model - INFO - {Epoch: 31} loss: 0.505665, accuracy: 0.823895, precision: 0.899790, recall: 0.752397, fmeasure: 0.817057, val_loss: 0.561864, val_accuracy: 0.866261, val_precision: 0.925172, val_recall: 0.783133, val_fmeasure: 0.846653\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.5057 - accuracy: 0.8239 - precision: 0.8998 - recall: 0.7524 - fmeasure: 0.8171 - val_loss: 0.5619 - val_accuracy: 0.8663 - val_precision: 0.9252 - val_recall: 0.7831 - val_fmeasure: 0.8467\n",
      "Epoch 33/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.4832 - accuracy: 0.8457 - precision: 0.9143 - recall: 0.7838 - fmeasure: 0.8421src.model - INFO - {Epoch: 32} loss: 0.487026, accuracy: 0.845499, precision: 0.914009, recall: 0.783551, fmeasure: 0.841861, val_loss: 0.514800, val_accuracy: 0.880243, val_precision: 0.928437, val_recall: 0.830121, val_fmeasure: 0.875337\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.4870 - accuracy: 0.8455 - precision: 0.9140 - recall: 0.7836 - fmeasure: 0.8419 - val_loss: 0.5148 - val_accuracy: 0.8802 - val_precision: 0.9284 - val_recall: 0.8301 - val_fmeasure: 0.8753\n",
      "Epoch 34/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.9753 - accuracy: 0.7334 - precision: 0.8444 - recall: 0.6298 - fmeasure: 0.7160src.model - INFO - {Epoch: 33} loss: 0.971395, accuracy: 0.732570, precision: 0.844099, recall: 0.630174, fmeasure: 0.716131, val_loss: 0.593173, val_accuracy: 0.823100, val_precision: 0.901130, val_recall: 0.724699, val_fmeasure: 0.800909\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.9714 - accuracy: 0.7326 - precision: 0.8441 - recall: 0.6302 - fmeasure: 0.7161 - val_loss: 0.5932 - val_accuracy: 0.8231 - val_precision: 0.9011 - val_recall: 0.7247 - val_fmeasure: 0.8009\n",
      "Epoch 35/67\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.5836 - accuracy: 0.8123 - precision: 0.8951 - recall: 0.7383 - fmeasure: 0.8069src.model - INFO - {Epoch: 34} loss: 0.580745, accuracy: 0.813421, precision: 0.895888, recall: 0.739869, fmeasure: 0.808133, val_loss: 0.528261, val_accuracy: 0.861398, val_precision: 0.917670, val_recall: 0.811446, val_fmeasure: 0.859716\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.5807 - accuracy: 0.8134 - precision: 0.8959 - recall: 0.7399 - fmeasure: 0.8081 - val_loss: 0.5283 - val_accuracy: 0.8614 - val_precision: 0.9177 - val_recall: 0.8114 - val_fmeasure: 0.8597\n",
      "Epoch 36/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.4337 - accuracy: 0.8616 - precision: 0.9149 - recall: 0.8106 - fmeasure: 0.8582src.model - INFO - {Epoch: 35} loss: 0.432210, accuracy: 0.862520, precision: 0.915565, recall: 0.811220, fmeasure: 0.858827, val_loss: 0.399773, val_accuracy: 0.890577, val_precision: 0.922618, val_recall: 0.839759, val_fmeasure: 0.877990\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.4322 - accuracy: 0.8625 - precision: 0.9156 - recall: 0.8112 - fmeasure: 0.8588 - val_loss: 0.3998 - val_accuracy: 0.8906 - val_precision: 0.9226 - val_recall: 0.8398 - val_fmeasure: 0.8780\n",
      "Epoch 37/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.4250 - accuracy: 0.8616 - precision: 0.9054 - recall: 0.8199 - fmeasure: 0.8593src.model - INFO - {Epoch: 36} loss: 0.423528, accuracy: 0.862520, precision: 0.905758, recall: 0.820370, fmeasure: 0.859745, val_loss: 0.409419, val_accuracy: 0.886322, val_precision: 0.919902, val_recall: 0.844578, val_fmeasure: 0.879794\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.4235 - accuracy: 0.8625 - precision: 0.9058 - recall: 0.8204 - fmeasure: 0.8597 - val_loss: 0.4094 - val_accuracy: 0.8863 - val_precision: 0.9199 - val_recall: 0.8446 - val_fmeasure: 0.8798\n",
      "Epoch 38/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.3682 - accuracy: 0.8699 - precision: 0.9167 - recall: 0.8308 - fmeasure: 0.8704src.model - INFO - {Epoch: 37} loss: 0.367818, accuracy: 0.869722, precision: 0.916293, recall: 0.831264, fmeasure: 0.870458, val_loss: 0.334960, val_accuracy: 0.911854, val_precision: 0.936531, val_recall: 0.876506, val_fmeasure: 0.904730\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3678 - accuracy: 0.8697 - precision: 0.9163 - recall: 0.8313 - fmeasure: 0.8705 - val_loss: 0.3350 - val_accuracy: 0.9119 - val_precision: 0.9365 - val_recall: 0.8765 - val_fmeasure: 0.9047\n",
      "Epoch 39/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.8897 - precision: 0.9261 - recall: 0.8467 - fmeasure: 0.8838src.model - INFO - {Epoch: 38} loss: 0.325970, accuracy: 0.890344, precision: 0.927081, recall: 0.847495, fmeasure: 0.884646, val_loss: 0.347506, val_accuracy: 0.921581, val_precision: 0.935389, val_recall: 0.885542, val_fmeasure: 0.909187\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3260 - accuracy: 0.8903 - precision: 0.9271 - recall: 0.8475 - fmeasure: 0.8846 - val_loss: 0.3475 - val_accuracy: 0.9216 - val_precision: 0.9354 - val_recall: 0.8855 - val_fmeasure: 0.9092\n",
      "Epoch 40/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.4196 - accuracy: 0.8811 - precision: 0.9152 - recall: 0.8487 - fmeasure: 0.8797src.model - INFO - {Epoch: 39} loss: 0.422033, accuracy: 0.880196, precision: 0.914035, recall: 0.847603, fmeasure: 0.878572, val_loss: 0.424081, val_accuracy: 0.894833, val_precision: 0.921133, val_recall: 0.862651, val_fmeasure: 0.890084\n",
      "153/153 [==============================] - 2s 15ms/step - loss: 0.4220 - accuracy: 0.8802 - precision: 0.9140 - recall: 0.8476 - fmeasure: 0.8786 - val_loss: 0.4241 - val_accuracy: 0.8948 - val_precision: 0.9211 - val_recall: 0.8627 - val_fmeasure: 0.8901\n",
      "Epoch 41/67\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.3743 - accuracy: 0.8774 - precision: 0.9157 - recall: 0.8334 - fmeasure: 0.8716src.model - INFO - {Epoch: 40} loss: 0.371905, accuracy: 0.877905, precision: 0.915508, recall: 0.834750, fmeasure: 0.872238, val_loss: 0.373450, val_accuracy: 0.912462, val_precision: 0.932514, val_recall: 0.884337, val_fmeasure: 0.907260\n",
      "153/153 [==============================] - 2s 13ms/step - loss: 0.3719 - accuracy: 0.8779 - precision: 0.9155 - recall: 0.8347 - fmeasure: 0.8722 - val_loss: 0.3735 - val_accuracy: 0.9125 - val_precision: 0.9325 - val_recall: 0.8843 - val_fmeasure: 0.9073\n",
      "Epoch 42/67\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.9082 - precision: 0.9325 - recall: 0.8760 - fmeasure: 0.9026src.model - INFO - {Epoch: 41} loss: 0.298435, accuracy: 0.908020, precision: 0.932072, recall: 0.875926, fmeasure: 0.902361, val_loss: 0.526977, val_accuracy: 0.887538, val_precision: 0.914885, val_recall: 0.863253, val_fmeasure: 0.887629\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2984 - accuracy: 0.9080 - precision: 0.9321 - recall: 0.8759 - fmeasure: 0.9024 - val_loss: 0.5270 - val_accuracy: 0.8875 - val_precision: 0.9149 - val_recall: 0.8633 - val_fmeasure: 0.8876\n",
      "Epoch 43/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.9007 - precision: 0.9299 - recall: 0.8722 - fmeasure: 0.8994src.model - INFO - {Epoch: 42} loss: 0.279055, accuracy: 0.899509, precision: 0.928469, recall: 0.871242, fmeasure: 0.898222, val_loss: 0.349386, val_accuracy: 0.918541, val_precision: 0.945286, val_recall: 0.893976, val_fmeasure: 0.918083\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2791 - accuracy: 0.8995 - precision: 0.9285 - recall: 0.8712 - fmeasure: 0.8982 - val_loss: 0.3494 - val_accuracy: 0.9185 - val_precision: 0.9453 - val_recall: 0.8940 - val_fmeasure: 0.9181\n",
      "Epoch 44/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.9056 - precision: 0.9338 - recall: 0.8772 - fmeasure: 0.9038src.model - INFO - {Epoch: 43} loss: 0.305027, accuracy: 0.905728, precision: 0.933959, recall: 0.877778, fmeasure: 0.904218, val_loss: 0.420606, val_accuracy: 0.898480, val_precision: 0.922568, val_recall: 0.884338, val_fmeasure: 0.902509\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3050 - accuracy: 0.9057 - precision: 0.9340 - recall: 0.8778 - fmeasure: 0.9042 - val_loss: 0.4206 - val_accuracy: 0.8985 - val_precision: 0.9226 - val_recall: 0.8843 - val_fmeasure: 0.9025\n",
      "Epoch 45/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9169 - precision: 0.9389 - recall: 0.8930 - fmeasure: 0.9149src.model - INFO - {Epoch: 44} loss: 0.234993, accuracy: 0.916858, precision: 0.938582, recall: 0.892592, fmeasure: 0.914461, val_loss: 0.372285, val_accuracy: 0.920973, val_precision: 0.935918, val_recall: 0.904217, val_fmeasure: 0.919434\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2350 - accuracy: 0.9169 - precision: 0.9386 - recall: 0.8926 - fmeasure: 0.9145 - val_loss: 0.3723 - val_accuracy: 0.9210 - val_precision: 0.9359 - val_recall: 0.9042 - val_fmeasure: 0.9194\n",
      "Epoch 46/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.3699 - accuracy: 0.9050 - precision: 0.9305 - recall: 0.8821 - fmeasure: 0.9049src.model - INFO - {Epoch: 45} loss: 0.372104, accuracy: 0.903764, precision: 0.929677, recall: 0.880937, fmeasure: 0.903913, val_loss: 0.449127, val_accuracy: 0.908815, val_precision: 0.922806, val_recall: 0.887350, val_fmeasure: 0.904274\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3721 - accuracy: 0.9038 - precision: 0.9297 - recall: 0.8809 - fmeasure: 0.9039 - val_loss: 0.4491 - val_accuracy: 0.9088 - val_precision: 0.9228 - val_recall: 0.8873 - val_fmeasure: 0.9043\n",
      "Epoch 47/67\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2818 - accuracy: 0.9092 - precision: 0.9324 - recall: 0.8832 - fmeasure: 0.9064src.model - INFO - {Epoch: 46} loss: 0.280554, accuracy: 0.909656, precision: 0.932800, recall: 0.883987, fmeasure: 0.906970, val_loss: 0.359310, val_accuracy: 0.915502, val_precision: 0.932071, val_recall: 0.902410, val_fmeasure: 0.916638\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2806 - accuracy: 0.9097 - precision: 0.9328 - recall: 0.8840 - fmeasure: 0.9070 - val_loss: 0.3593 - val_accuracy: 0.9155 - val_precision: 0.9321 - val_recall: 0.9024 - val_fmeasure: 0.9166\n",
      "Epoch 48/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2726 - accuracy: 0.9146 - precision: 0.9366 - recall: 0.8974 - fmeasure: 0.9160src.model - INFO - {Epoch: 47} loss: 0.273555, accuracy: 0.914239, precision: 0.936554, recall: 0.896841, fmeasure: 0.915741, val_loss: 0.390566, val_accuracy: 0.916717, val_precision: 0.932387, val_recall: 0.895181, val_fmeasure: 0.912807\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2736 - accuracy: 0.9142 - precision: 0.9366 - recall: 0.8968 - fmeasure: 0.9157 - val_loss: 0.3906 - val_accuracy: 0.9167 - val_precision: 0.9324 - val_recall: 0.8952 - val_fmeasure: 0.9128\n",
      "Epoch 49/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.6662 - accuracy: 0.8579 - precision: 0.8906 - recall: 0.8291 - fmeasure: 0.8575src.model - INFO - {Epoch: 48} loss: 0.662467, accuracy: 0.858265, precision: 0.890802, recall: 0.829085, fmeasure: 0.857564, val_loss: 0.660718, val_accuracy: 0.829179, val_precision: 0.878422, val_recall: 0.781325, val_fmeasure: 0.825549\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.6625 - accuracy: 0.8583 - precision: 0.8908 - recall: 0.8291 - fmeasure: 0.8576 - val_loss: 0.6607 - val_accuracy: 0.8292 - val_precision: 0.8784 - val_recall: 0.7813 - val_fmeasure: 0.8255\n",
      "Epoch 50/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.5438 - accuracy: 0.8331 - precision: 0.8842 - recall: 0.7901 - fmeasure: 0.8329src.model - INFO - {Epoch: 49} loss: 0.540192, accuracy: 0.834043, precision: 0.884628, recall: 0.791285, fmeasure: 0.833706, val_loss: 0.540091, val_accuracy: 0.885106, val_precision: 0.921406, val_recall: 0.865663, val_fmeasure: 0.891913\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.5402 - accuracy: 0.8340 - precision: 0.8846 - recall: 0.7913 - fmeasure: 0.8337 - val_loss: 0.5401 - val_accuracy: 0.8851 - val_precision: 0.9214 - val_recall: 0.8657 - val_fmeasure: 0.8919\n",
      "Epoch 51/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.3799 - accuracy: 0.8848 - precision: 0.9164 - recall: 0.8560 - fmeasure: 0.8843src.model - INFO - {Epoch: 50} loss: 0.380453, accuracy: 0.884124, precision: 0.915290, recall: 0.855011, fmeasure: 0.883262, val_loss: 0.465005, val_accuracy: 0.899088, val_precision: 0.920537, val_recall: 0.871084, val_fmeasure: 0.894498\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3805 - accuracy: 0.8841 - precision: 0.9153 - recall: 0.8550 - fmeasure: 0.8833 - val_loss: 0.4650 - val_accuracy: 0.8991 - val_precision: 0.9205 - val_recall: 0.8711 - val_fmeasure: 0.8945\n",
      "Epoch 52/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.3217 - accuracy: 0.9086 - precision: 0.9323 - recall: 0.8755 - fmeasure: 0.9022src.model - INFO - {Epoch: 51} loss: 0.321804, accuracy: 0.908674, precision: 0.933159, recall: 0.875599, fmeasure: 0.902629, val_loss: 0.515713, val_accuracy: 0.901520, val_precision: 0.923735, val_recall: 0.886145, val_fmeasure: 0.904148\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3218 - accuracy: 0.9087 - precision: 0.9332 - recall: 0.8756 - fmeasure: 0.9026 - val_loss: 0.5157 - val_accuracy: 0.9015 - val_precision: 0.9237 - val_recall: 0.8861 - val_fmeasure: 0.9041\n",
      "Epoch 53/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.9116 - precision: 0.9322 - recall: 0.8891 - fmeasure: 0.9096src.model - INFO - {Epoch: 52} loss: 0.323694, accuracy: 0.910311, precision: 0.930802, recall: 0.887909, fmeasure: 0.908366, val_loss: 0.588852, val_accuracy: 0.910030, val_precision: 0.931320, val_recall: 0.898193, val_fmeasure: 0.913962\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3237 - accuracy: 0.9103 - precision: 0.9308 - recall: 0.8879 - fmeasure: 0.9084 - val_loss: 0.5889 - val_accuracy: 0.9100 - val_precision: 0.9313 - val_recall: 0.8982 - val_fmeasure: 0.9140\n",
      "Epoch 54/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.9089 - precision: 0.9343 - recall: 0.8834 - fmeasure: 0.9075src.model - INFO - {Epoch: 53} loss: 0.317534, accuracy: 0.909329, precision: 0.934525, recall: 0.884313, fmeasure: 0.908086, val_loss: 0.434352, val_accuracy: 0.910638, val_precision: 0.935681, val_recall: 0.888554, val_fmeasure: 0.910796\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3175 - accuracy: 0.9093 - precision: 0.9345 - recall: 0.8843 - fmeasure: 0.9081 - val_loss: 0.4344 - val_accuracy: 0.9106 - val_precision: 0.9357 - val_recall: 0.8886 - val_fmeasure: 0.9108\n",
      "Epoch 55/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2142 - accuracy: 0.9295 - precision: 0.9467 - recall: 0.9109 - fmeasure: 0.9280src.model - INFO - {Epoch: 54} loss: 0.214559, accuracy: 0.928969, precision: 0.946249, recall: 0.910349, fmeasure: 0.927480, val_loss: 0.472489, val_accuracy: 0.925228, val_precision: 0.939337, val_recall: 0.910241, val_fmeasure: 0.924239\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2146 - accuracy: 0.9290 - precision: 0.9462 - recall: 0.9103 - fmeasure: 0.9275 - val_loss: 0.4725 - val_accuracy: 0.9252 - val_precision: 0.9393 - val_recall: 0.9102 - val_fmeasure: 0.9242\n",
      "Epoch 56/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2218 - accuracy: 0.9308 - precision: 0.9485 - recall: 0.9109 - fmeasure: 0.9289src.model - INFO - {Epoch: 55} loss: 0.222019, accuracy: 0.930933, precision: 0.948417, recall: 0.910893, fmeasure: 0.928851, val_loss: 0.359752, val_accuracy: 0.906991, val_precision: 0.918385, val_recall: 0.889759, val_fmeasure: 0.903510\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2220 - accuracy: 0.9309 - precision: 0.9484 - recall: 0.9109 - fmeasure: 0.9289 - val_loss: 0.3598 - val_accuracy: 0.9070 - val_precision: 0.9184 - val_recall: 0.8898 - val_fmeasure: 0.9035\n",
      "Epoch 57/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9334 - precision: 0.9486 - recall: 0.9159 - fmeasure: 0.9316src.model - INFO - {Epoch: 56} loss: 0.189504, accuracy: 0.933224, precision: 0.948454, recall: 0.915795, fmeasure: 0.931476, val_loss: 0.518323, val_accuracy: 0.930699, val_precision: 0.943416, val_recall: 0.922289, val_fmeasure: 0.932423\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.1895 - accuracy: 0.9332 - precision: 0.9485 - recall: 0.9158 - fmeasure: 0.9315 - val_loss: 0.5183 - val_accuracy: 0.9307 - val_precision: 0.9434 - val_recall: 0.9223 - val_fmeasure: 0.9324\n",
      "Epoch 58/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.9368 - precision: 0.9501 - recall: 0.9202 - fmeasure: 0.9344src.model - INFO - {Epoch: 57} loss: 0.222407, accuracy: 0.937152, precision: 0.950272, recall: 0.920806, fmeasure: 0.934860, val_loss: 0.439843, val_accuracy: 0.911246, val_precision: 0.929698, val_recall: 0.904217, val_fmeasure: 0.916396\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2224 - accuracy: 0.9372 - precision: 0.9503 - recall: 0.9208 - fmeasure: 0.9349 - val_loss: 0.4398 - val_accuracy: 0.9112 - val_precision: 0.9297 - val_recall: 0.9042 - val_fmeasure: 0.9164\n",
      "Epoch 59/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.3468 - accuracy: 0.9162 - precision: 0.9310 - recall: 0.8974 - fmeasure: 0.9135src.model - INFO - {Epoch: 58} loss: 0.347793, accuracy: 0.914894, precision: 0.930336, recall: 0.896296, fmeasure: 0.912566, val_loss: 0.500403, val_accuracy: 0.905775, val_precision: 0.927585, val_recall: 0.890361, val_fmeasure: 0.908005\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3478 - accuracy: 0.9149 - precision: 0.9303 - recall: 0.8963 - fmeasure: 0.9126 - val_loss: 0.5004 - val_accuracy: 0.9058 - val_precision: 0.9276 - val_recall: 0.8904 - val_fmeasure: 0.9080\n",
      "Epoch 60/67\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2782 - accuracy: 0.9237 - precision: 0.9422 - recall: 0.9003 - fmeasure: 0.9202src.model - INFO - {Epoch: 59} loss: 0.277292, accuracy: 0.923732, precision: 0.942595, recall: 0.900545, fmeasure: 0.920514, val_loss: 0.655759, val_accuracy: 0.913070, val_precision: 0.931053, val_recall: 0.904217, val_fmeasure: 0.917156\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2773 - accuracy: 0.9237 - precision: 0.9426 - recall: 0.9005 - fmeasure: 0.9205 - val_loss: 0.6558 - val_accuracy: 0.9131 - val_precision: 0.9311 - val_recall: 0.9042 - val_fmeasure: 0.9172\n",
      "Epoch 61/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy: 0.9407 - precision: 0.9533 - recall: 0.9232 - fmeasure: 0.9377src.model - INFO - {Epoch: 60} loss: 0.211853, accuracy: 0.941080, precision: 0.953588, recall: 0.923203, fmeasure: 0.937786, val_loss: 0.551330, val_accuracy: 0.924620, val_precision: 0.936009, val_recall: 0.909639, val_fmeasure: 0.922365\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.2119 - accuracy: 0.9411 - precision: 0.9536 - recall: 0.9232 - fmeasure: 0.9378 - val_loss: 0.5513 - val_accuracy: 0.9246 - val_precision: 0.9360 - val_recall: 0.9096 - val_fmeasure: 0.9224\n",
      "Epoch 62/67\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.9263 - precision: 0.9419 - recall: 0.9132 - fmeasure: 0.9270src.model - INFO - {Epoch: 61} loss: 0.340599, accuracy: 0.926678, precision: 0.942303, recall: 0.913725, fmeasure: 0.927466, val_loss: 0.396543, val_accuracy: 0.921581, val_precision: 0.935364, val_recall: 0.910241, val_fmeasure: 0.922289\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3406 - accuracy: 0.9267 - precision: 0.9423 - recall: 0.9137 - fmeasure: 0.9275 - val_loss: 0.3965 - val_accuracy: 0.9216 - val_precision: 0.9354 - val_recall: 0.9102 - val_fmeasure: 0.9223\n",
      "Epoch 63/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.9116 - precision: 0.9298 - recall: 0.8921 - fmeasure: 0.9100src.model - INFO - {Epoch: 62} loss: 0.335955, accuracy: 0.911620, precision: 0.929825, recall: 0.892157, fmeasure: 0.910000, val_loss: 0.469360, val_accuracy: 0.917325, val_precision: 0.931741, val_recall: 0.902410, val_fmeasure: 0.916509\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.3360 - accuracy: 0.9116 - precision: 0.9298 - recall: 0.8922 - fmeasure: 0.9100 - val_loss: 0.4694 - val_accuracy: 0.9173 - val_precision: 0.9317 - val_recall: 0.9024 - val_fmeasure: 0.9165\n",
      "Epoch 64/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.1887 - accuracy: 0.9397 - precision: 0.9504 - recall: 0.9215 - fmeasure: 0.9353src.model - INFO - {Epoch: 63} loss: 0.187724, accuracy: 0.940098, precision: 0.950679, recall: 0.922222, fmeasure: 0.935839, val_loss: 0.437690, val_accuracy: 0.927660, val_precision: 0.936127, val_recall: 0.921084, val_fmeasure: 0.928369\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.1877 - accuracy: 0.9401 - precision: 0.9507 - recall: 0.9222 - fmeasure: 0.9358 - val_loss: 0.4377 - val_accuracy: 0.9277 - val_precision: 0.9361 - val_recall: 0.9211 - val_fmeasure: 0.9284\n",
      "Epoch 65/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9454 - precision: 0.9585 - recall: 0.9331 - fmeasure: 0.9454src.model - INFO - {Epoch: 64} loss: 0.161491, accuracy: 0.945008, precision: 0.958324, recall: 0.932679, fmeasure: 0.945063, val_loss: 0.427708, val_accuracy: 0.930091, val_precision: 0.939234, val_recall: 0.923494, val_fmeasure: 0.931102\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.1615 - accuracy: 0.9450 - precision: 0.9583 - recall: 0.9327 - fmeasure: 0.9451 - val_loss: 0.4277 - val_accuracy: 0.9301 - val_precision: 0.9392 - val_recall: 0.9235 - val_fmeasure: 0.9311\n",
      "Epoch 66/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.1753 - accuracy: 0.9474 - precision: 0.9572 - recall: 0.9374 - fmeasure: 0.9470src.model - INFO - {Epoch: 65} loss: 0.175611, accuracy: 0.946645, precision: 0.956963, recall: 0.936601, fmeasure: 0.946419, val_loss: 0.571063, val_accuracy: 0.930091, val_precision: 0.942981, val_recall: 0.912048, val_fmeasure: 0.926841\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.1756 - accuracy: 0.9466 - precision: 0.9570 - recall: 0.9366 - fmeasure: 0.9464 - val_loss: 0.5711 - val_accuracy: 0.9301 - val_precision: 0.9430 - val_recall: 0.9120 - val_fmeasure: 0.9268\n",
      "Epoch 67/67\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.1827 - accuracy: 0.9483 - precision: 0.9593 - recall: 0.9318 - fmeasure: 0.9450src.model - INFO - {Epoch: 66} loss: 0.182332, accuracy: 0.947954, precision: 0.958648, recall: 0.931481, fmeasure: 0.944500, val_loss: 0.525666, val_accuracy: 0.936170, val_precision: 0.943113, val_recall: 0.929518, val_fmeasure: 0.936111\n",
      "153/153 [==============================] - 2s 14ms/step - loss: 0.1823 - accuracy: 0.9480 - precision: 0.9586 - recall: 0.9315 - fmeasure: 0.9445 - val_loss: 0.5257 - val_accuracy: 0.9362 - val_precision: 0.9431 - val_recall: 0.9295 - val_fmeasure: 0.9361\n",
      "src.model - INFO - Training completed\n",
      "src.model - INFO - Evaluating model\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5257 - accuracy: 0.9362 - precision: 0.9424 - recall: 0.9288 - fmeasure: 0.9355\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0448 - accuracy: 0.9879 - precision: 0.9895 - recall: 0.9847 - fmeasure: 0.9871\n",
      "src.model - INFO - Train loss: 0.04479433596134186\n",
      "src.model - INFO - Train precision: 0.9895195960998535\n",
      "src.model - INFO - Train recall: 0.9847005009651184\n",
      "src.model - INFO - Train f1-score: 0.9870612025260925\n",
      "src.model - INFO - Test loss: 0.5256662368774414\n",
      "src.model - INFO - Test precision: 0.9424282312393188\n",
      "src.model - INFO - Test recall: 0.9288091659545898\n",
      "src.model - INFO - Test f1-score: 0.9354735612869263\n",
      "52/52 [==============================] - 0s 5ms/step\n",
      "src.train - INFO - Confusion Matrix for classes ['a', 'am', 'bm', 'c', 'd', 'dm', 'e', 'em', 'f', 'g']:\n",
      "[[150   1   1   0   0   1   2   1   1   1]\n",
      " [  7 164   2   0   0   0   0   0   1   0]\n",
      " [  0   0 146   0   1   0   1   0   0   0]\n",
      " [  1   1   1 146   2   0   0   0   1   2]\n",
      " [  0   1   2   2 142   5   2   0   1   1]\n",
      " [  1   0   1   1  17 147   1   6   1   5]\n",
      " [  0   0   0   2   0   0 160   4   1   1]\n",
      " [  0   0   0   3   2   2   3 163   1   1]\n",
      " [  0   0   0   0   1   0   0   0 140   2]\n",
      " [  0   1   1   2   1   0   1   1   0 182]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import keras\n",
    "import os, glob\n",
    "import logging\n",
    "import librosa, librosa.display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "from src.metrics import *\n",
    "from settings import *\n",
    "from src.data import generate\n",
    "from src.processing import *\n",
    "from src.model import CNN\n",
    "from src.data.preprocessing import get_most_shape\n",
    "from setup_logging import setup_logging\n",
    "\n",
    "setup_logging()\n",
    "logger = logging.getLogger('src.train')\n",
    "\n",
    "train_datas = []\n",
    "test_datas = []\n",
    "\n",
    "for dataset in datasets_raw:\n",
    "    train_data, test_data = train_test_split(dataset, augmented=False, split_ratio=0.65)\n",
    "    train_datas.append(train_data)\n",
    "    test_datas.append(test_data)\n",
    "\n",
    "for dataset in datasets_augmented:\n",
    "    train_data, test_data = train_test_split(dataset, augmented=False, split_ratio=0.65)\n",
    "    train_datas.append(train_data)\n",
    "    test_datas.append(test_data)\n",
    "\n",
    "train_data = pd.concat(train_datas)\n",
    "test_data = pd.concat(test_datas)\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "most_shape = get_most_shape(train_data)\n",
    "\n",
    "logger.info(f\"Number of train samples: {len(train_data)}\")\n",
    "logger.info(f\"Number of test samples: {len(test_data)}\")\n",
    "# most_shape = get_most_shape(dataset)\n",
    "#train_data, test_data = train_test_split(dataset, augmented=augmented, split_ratio=0.65)\n",
    "\n",
    "X_train, y_train = features_target_split(train_data)\n",
    "X_test, y_test = features_target_split(test_data)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train, X_test = reshape_feature_CNN(X_train, size=max_spectrogram_size), reshape_feature_CNN(X_test, size=max_spectrogram_size)\n",
    "\n",
    "# Preserve y_test values\n",
    "y_test_values = y_test.copy()\n",
    "\n",
    "# One-Hot encoding for classes\n",
    "y_train, y_test = one_hot_encode(y_train), one_hot_encode(y_test)\n",
    "\n",
    "# Instance of CNN model\n",
    "cnn = CNN(most_shape)\n",
    "logger.info(str(cnn))\n",
    "\n",
    "cnn.train(X_train, y_train, X_test, y_test)\n",
    "cnn.evaluate(X_train, y_train, X_test, y_test)\n",
    "\n",
    "if tf.__version__ != '1.8.0':\n",
    "    predict_x=cnn.model.predict(X_test)\n",
    "    predictions = np.argmax(predict_x,axis=1)\n",
    "else:    \n",
    "    predictions = cnn.model.predict_classes(X_test)\n",
    "conf_matrix=confusion_matrix(y_test_values, predictions, labels=range(10))\n",
    "logger.info('Confusion Matrix for classes {}:\\n{}'.format(CLASSES, conf_matrix))\n",
    "#cnn.save_model(name=\"model_all_data_augment_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1625 - accuracy: 0.9529 - precision: 0.9569 - recall: 0.9458 - fmeasure: 0.9512\n",
      "Test score for instrument: Guitar\n",
      "\tTest loss: 0.16254399716854095\n",
      "\tTest accuracy: 0.9528571367263794\n",
      "\tTest precision: 0.9569084644317627\n",
      "\tTest recall: 0.9458197951316833\n",
      "\tTest f1-score: 0.9511818289756775\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.4948 - accuracy: 0.9397 - precision: 0.9448 - recall: 0.9281 - fmeasure: 0.9362\n",
      "Test score for instrument: Accordion\n",
      "\tTest loss: 1.49477219581604\n",
      "\tTest accuracy: 0.9396825432777405\n",
      "\tTest precision: 0.9447916746139526\n",
      "\tTest recall: 0.9281250238418579\n",
      "\tTest f1-score: 0.9361894726753235\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2296 - accuracy: 0.9524 - precision: 0.9585 - recall: 0.9500 - fmeasure: 0.9542\n",
      "Test score for instrument: Violin\n",
      "\tTest loss: 0.2296053022146225\n",
      "\tTest accuracy: 0.9523809552192688\n",
      "\tTest precision: 0.958548367023468\n",
      "\tTest recall: 0.949999988079071\n",
      "\tTest f1-score: 0.9541603326797485\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6596 - accuracy: 0.8794 - precision: 0.8868 - recall: 0.8750 - fmeasure: 0.8807\n",
      "Test score for instrument: Piano\n",
      "\tTest loss: 0.6595577001571655\n",
      "\tTest accuracy: 0.879365086555481\n",
      "\tTest precision: 0.8868221044540405\n",
      "\tTest recall: 0.875\n",
      "\tTest f1-score: 0.8806522488594055\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_instruments = instruments + instruments_aug\n",
    "for test_data, instrument in zip(test_datas, test_instruments):\n",
    "    X_test = test_data['spectrogram']\n",
    "    X_test = np.array([x.reshape( (128, max_spectrogram_size, 1) ) for x in X_test])\n",
    "    y_test = test_data['class_ID']\n",
    "\n",
    "    y_test_values=y_test\n",
    "    y_test = np.array(keras.utils.to_categorical(y_test, 10))\n",
    "\n",
    "    score = cnn.model.evaluate(X_test,y_test)\n",
    "    print(f'Test score for instrument: {instrument}')\n",
    "    print('\\tTest loss:', score[0])\n",
    "    print('\\tTest accuracy:', score[1])\n",
    "    print('\\tTest precision:', score[2])\n",
    "    print('\\tTest recall:', score[3])\n",
    "    print('\\tTest f1-score:', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.model - INFO - Saving model\n",
      "src.model - INFO - Saved model to /home/tzag/danigil/dl/guitarCR/models\n"
     ]
    }
   ],
   "source": [
    "cnn.save_model(name=\"model_alldata_augment_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 128, 87, 1), found shape=(None, 128, 107, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m y_test_values\u001b[39m=\u001b[39my_test\n\u001b[1;32m     18\u001b[0m y_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(y_test, \u001b[39m10\u001b[39m))\n\u001b[0;32m---> 20\u001b[0m score \u001b[39m=\u001b[39m baseline_model\u001b[39m.\u001b[39;49mevaluate(X_test,y_test)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTest score for instrument: \u001b[39m\u001b[39m{\u001b[39;00minstrument\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[39m# print('\\tTest loss:', score[0])\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# print('\\tTest accuracy:', score[1])\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# print('\\tTest precision:', score[2])\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# print('\\tTest recall:', score[3])\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filekcblrgyb.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 128, 87, 1), found shape=(None, 128, 107, 1)\n"
     ]
    }
   ],
   "source": [
    "with open(MODEL_1_JSON, \"r\") as json_file:\n",
    "\tloaded_model_json = json_file.read()\n",
    "\n",
    "baseline_model = model_from_json(loaded_model_json)\n",
    "baseline_model.load_weights(MODEL_1_H5)\n",
    "\n",
    "baseline_model.compile(\n",
    "            optimizer=\"Adam\",\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=['accuracy', precision, recall, fmeasure])\n",
    "\n",
    "for test_data, instrument in zip(test_datas, test_instruments):\n",
    "    X_test = test_data['spectrogram']\n",
    "    X_test = np.array([x.reshape( (128, max_spectrogram_size, 1) ) for x in X_test])\n",
    "    y_test = test_data['class_ID']\n",
    "\n",
    "    y_test_values=y_test\n",
    "    y_test = np.array(keras.utils.to_categorical(y_test, 10))\n",
    "\n",
    "    score = baseline_model.evaluate(X_test,y_test)\n",
    "    print(f'Test score for instrument: {instrument}')\n",
    "    # print('\\tTest loss:', score[0])\n",
    "    # print('\\tTest accuracy:', score[1])\n",
    "    # print('\\tTest precision:', score[2])\n",
    "    # print('\\tTest recall:', score[3])\n",
    "    print('\\tf1-score:', score[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danigil-steganalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
