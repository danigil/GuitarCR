{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 14:21:05.858883: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 14:21:05.985031: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-19 14:21:06.629821: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-19 14:21:06.629896: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-19 14:21:06.629903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "from src.metrics import *\n",
    "from settings import *\n",
    "from src.data import generate\n",
    "\n",
    "instruments = ['Guitar']\n",
    "instruments_aug = ['Accordion', 'Violin', 'Piano']\n",
    "\n",
    "# from setup_logging import setup_logging\n",
    "# setup_logging()\n",
    "\n",
    "#generate.my_run(instruments)\n",
    "datasets_raw = [pd.read_pickle(os.path.join(METADATA_DIR_PROCESSED, f'data_{instrument.lower()}.pkl')) for instrument in instruments]\n",
    "datasets_augmented = [pd.read_pickle(os.path.join(METADATA_DIR_AUGMENTED_RAW, f'data_{instrument.lower()}.pkl')) for instrument in instruments_aug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 60), (128, 65), (128, 73), (128, 76), (128, 61), (128, 72), (128, 69), (128, 58), (128, 51), (128, 65), (128, 54), (128, 72), (128, 81), (128, 61), (128, 65), (128, 35), (128, 65), (128, 81), (128, 21), (128, 73), (128, 67), (128, 27), (128, 81), (128, 74), (128, 78), (128, 47), (128, 81), (128, 81), (128, 81), (128, 81), (128, 55), (128, 46), (128, 66), (128, 73), (128, 53), (128, 78), (128, 68), (128, 60), (128, 67), (128, 59), (128, 81), (128, 79), (128, 64), (128, 75), (128, 81), (128, 81), (128, 81), (128, 39), (128, 70), (128, 81), (128, 81), (128, 81), (128, 47), (128, 81), (128, 68), (128, 81), (128, 81), (128, 81), (128, 46), (128, 40), (128, 81), (128, 75), (128, 73), (128, 61), (128, 44), (128, 81), (128, 81), (128, 73), (128, 81), (128, 81), (128, 25), (128, 72), (128, 51), (128, 81), (128, 81), (128, 81), (128, 66), (128, 65), (128, 52), (128, 62), (128, 81), (128, 72), (128, 81), (128, 57), (128, 65), (128, 71), (128, 76), (128, 26), (128, 56), (128, 81), (128, 50), (128, 46), (128, 47), (128, 45), (128, 48), (128, 58), (128, 48), (128, 46), (128, 74), (128, 39), (128, 79), (128, 86), (128, 97), (128, 100), (128, 81), (128, 95), (128, 91), (128, 77), (128, 67), (128, 85), (128, 71), (128, 95), (128, 107), (128, 81), (128, 86), (128, 46), (128, 86), (128, 107), (128, 27), (128, 96), (128, 89), (128, 35), (128, 107), (128, 98), (128, 103), (128, 62), (128, 107), (128, 107), (128, 107), (128, 107), (128, 72), (128, 60), (128, 88), (128, 96), (128, 70), (128, 103), (128, 90), (128, 80), (128, 89), (128, 78), (128, 107), (128, 104), (128, 84), (128, 99), (128, 107), (128, 107), (128, 107), (128, 52), (128, 92), (128, 107), (128, 107), (128, 107), (128, 62), (128, 107), (128, 90), (128, 107), (128, 107), (128, 107), (128, 60), (128, 53), (128, 107), (128, 99), (128, 97), (128, 81), (128, 59), (128, 107), (128, 107), (128, 96), (128, 107), (128, 107), (128, 33), (128, 95), (128, 67), (128, 107), (128, 107), (128, 107), (128, 88), (128, 86), (128, 68), (128, 81), (128, 107), (128, 96), (128, 107), (128, 75), (128, 86), (128, 93), (128, 101), (128, 34), (128, 73), (128, 107), (128, 65), (128, 60), (128, 62), (128, 60), (128, 63), (128, 76), (128, 63), (128, 61), (128, 98), (128, 52), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42), (128, 64), (128, 70), (128, 79), (128, 81), (128, 66), (128, 77), (128, 74), (128, 63), (128, 55), (128, 69), (128, 58), (128, 77), (128, 87), (128, 66), (128, 70), (128, 37), (128, 70), (128, 87), (128, 22), (128, 78), (128, 72), (128, 29), (128, 87), (128, 79), (128, 84), (128, 51), (128, 87), (128, 87), (128, 87), (128, 87), (128, 59), (128, 49), (128, 71), (128, 78), (128, 57), (128, 84), (128, 73), (128, 65), (128, 72), (128, 63), (128, 87), (128, 84), (128, 69), (128, 80), (128, 87), (128, 87), (128, 87), (128, 42), (128, 75), (128, 87), (128, 87), (128, 87), (128, 51), (128, 87), (128, 73), (128, 87), (128, 87), (128, 87), (128, 49), (128, 43), (128, 87), (128, 81), (128, 79), (128, 66), (128, 48), (128, 87), (128, 87), (128, 78), (128, 87), (128, 87), (128, 27), (128, 77), (128, 55), (128, 87), (128, 87), (128, 87), (128, 71), (128, 70), (128, 56), (128, 66), (128, 87), (128, 78), (128, 87), (128, 61), (128, 70), (128, 76), (128, 82), (128, 28), (128, 60), (128, 87), (128, 53), (128, 49), (128, 51), (128, 49), (128, 52), (128, 62), (128, 52), (128, 50), (128, 80), (128, 42)]\n"
     ]
    }
   ],
   "source": [
    "print([x.shape for x in datasets_augmented[0]['spectrogram']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.preprocessing import get_max_shape\n",
    "max_spectrogram_size = max(map(lambda df: get_max_shape(df), datasets_raw+datasets_augmented))\n",
    "max_spectrogram_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.preprocessing import uniform_shape\n",
    "uniform = lambda df: uniform_shape(df, max_spectrogram_size)\n",
    "\n",
    "datasets_raw = list(map(uniform,datasets_raw))\n",
    "datasets_augmented = list(map(uniform,datasets_augmented))\n",
    "datasets_augmented = list(map(lambda df: df[['spectrogram','class_ID', 'class_name','augmentation']],datasets_augmented))\n",
    "datasets_augmented = list(map(lambda df: df.reset_index(drop=True), datasets_augmented))\n",
    "#datasets_augmented = list(map(lambda df: df[['spectrogram','class_ID', 'class_name','augmentation']],map(uniform,datasets_augmented)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 107)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.preprocessing import all_equal\n",
    "#equal_shape = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.processing - INFO - Start train test split with split ratio: 0.65\n",
      "src.processing - INFO - Number of training samples is 1300\n",
      "src.processing - INFO - Number of testing samples is 700\n",
      "src.processing - INFO - Train test split completed\n",
      "src.processing - INFO - Start train test split with split ratio: 0.65\n",
      "src.processing - INFO - Number of training samples is 585\n",
      "src.processing - INFO - Number of testing samples is 315\n",
      "src.processing - INFO - Train test split completed\n",
      "src.processing - INFO - Start train test split with split ratio: 0.65\n",
      "src.processing - INFO - Number of training samples is 585\n",
      "src.processing - INFO - Number of testing samples is 315\n",
      "src.processing - INFO - Train test split completed\n",
      "src.processing - INFO - Start train test split with split ratio: 0.65\n",
      "src.processing - INFO - Number of training samples is 585\n",
      "src.processing - INFO - Number of testing samples is 315\n",
      "src.processing - INFO - Train test split completed\n",
      "The most frequent shape is (128, 107)\n",
      "src.train - INFO - Number of train samples: 3055\n",
      "src.train - INFO - Number of test samples: 1645\n",
      "src.processing - INFO - Start feature target split\n",
      "src.processing - INFO - Feature target split completed\n",
      "src.processing - INFO - Start feature target split\n",
      "src.processing - INFO - Feature target split completed\n",
      "src.processing - INFO - Features reshaped for CNN Input\n",
      "src.processing - INFO - Features reshaped for CNN Input\n",
      "src.processing - INFO - Target one hot encoded\n",
      "src.processing - INFO - Target one hot encoded\n",
      "src.model - INFO - Initializing CNN\n",
      "src.model - INFO - Input shape = (128, 107, 1)\n",
      "src.model - INFO - CNN Initialized\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 124, 103, 24)      624       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 31, 51, 24)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 31, 51, 24)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 27, 47, 48)        28848     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 23, 48)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 6, 23, 48)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 2, 19, 48)         57648     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 2, 19, 48)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1824)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1824)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                116800    \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 204,570\n",
      "Trainable params: 204,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "src.train - INFO - None\n",
      "src.model - INFO - Start training model\n",
      "src.model - INFO - Tensorboard Logging Started\n",
      "src.model - INFO - Use the following command in the terminal to view the logs during training: tensorboard --logdir logs/training\n",
      "Epoch 1/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 2.6686 - accuracy: 0.1342 - precision: 0.1980 - recall: 0.0190 - fmeasure: 0.0335src.model - INFO - {Epoch: 0} loss: 2.668610, accuracy: 0.134206, precision: 0.197993, recall: 0.019001, fmeasure: 0.033541, val_loss: 2.097650, val_accuracy: 0.213374, val_precision: 0.503236, val_recall: 0.046723, val_fmeasure: 0.083943\n",
      "191/191 [==============================] - 5s 16ms/step - loss: 2.6686 - accuracy: 0.1342 - precision: 0.1980 - recall: 0.0190 - fmeasure: 0.0335 - val_loss: 2.0976 - val_accuracy: 0.2134 - val_precision: 0.5032 - val_recall: 0.0467 - val_fmeasure: 0.0839\n",
      "Epoch 2/67\n",
      "190/191 [============================>.] - ETA: 0s - loss: 2.1357 - accuracy: 0.2023 - precision: 0.5478 - recall: 0.0622 - fmeasure: 0.1091src.model - INFO - {Epoch: 1} loss: 2.135998, accuracy: 0.202291, precision: 0.550174, recall: 0.062195, fmeasure: 0.109166, val_loss: 2.012946, val_accuracy: 0.266869, val_precision: 0.718608, val_recall: 0.097087, val_fmeasure: 0.165909\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 2.1360 - accuracy: 0.2023 - precision: 0.5502 - recall: 0.0622 - fmeasure: 0.1092 - val_loss: 2.0129 - val_accuracy: 0.2669 - val_precision: 0.7186 - val_recall: 0.0971 - val_fmeasure: 0.1659\n",
      "Epoch 3/67\n",
      "188/191 [============================>.] - ETA: 0s - loss: 2.0120 - accuracy: 0.2673 - precision: 0.6842 - recall: 0.1084 - fmeasure: 0.1804src.model - INFO - {Epoch: 2} loss: 2.009675, accuracy: 0.266776, precision: 0.682149, recall: 0.108682, fmeasure: 0.180781, val_loss: 1.804930, val_accuracy: 0.324620, val_precision: 0.907929, val_recall: 0.183532, val_fmeasure: 0.296684\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 2.0097 - accuracy: 0.2668 - precision: 0.6821 - recall: 0.1087 - fmeasure: 0.1808 - val_loss: 1.8049 - val_accuracy: 0.3246 - val_precision: 0.9079 - val_recall: 0.1835 - val_fmeasure: 0.2967\n",
      "Epoch 4/67\n",
      "187/191 [============================>.] - ETA: 0s - loss: 1.9050 - accuracy: 0.3095 - precision: 0.7755 - recall: 0.1578 - fmeasure: 0.2560src.model - INFO - {Epoch: 3} loss: 1.906770, accuracy: 0.307692, precision: 0.774084, recall: 0.157766, fmeasure: 0.255852, val_loss: 1.843538, val_accuracy: 0.346505, val_precision: 0.859778, val_recall: 0.176858, val_fmeasure: 0.286337\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.9068 - accuracy: 0.3077 - precision: 0.7741 - recall: 0.1578 - fmeasure: 0.2559 - val_loss: 1.8435 - val_accuracy: 0.3465 - val_precision: 0.8598 - val_recall: 0.1769 - val_fmeasure: 0.2863\n",
      "Epoch 5/67\n",
      "187/191 [============================>.] - ETA: 0s - loss: 1.8057 - accuracy: 0.3469 - precision: 0.8370 - recall: 0.1932 - fmeasure: 0.3040src.model - INFO - {Epoch: 4} loss: 1.810843, accuracy: 0.345336, precision: 0.826062, recall: 0.190467, fmeasure: 0.299848, val_loss: 1.702911, val_accuracy: 0.421885, val_precision: 0.863026, val_recall: 0.225681, val_fmeasure: 0.350355\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.8108 - accuracy: 0.3453 - precision: 0.8261 - recall: 0.1905 - fmeasure: 0.2998 - val_loss: 1.7029 - val_accuracy: 0.4219 - val_precision: 0.8630 - val_recall: 0.2257 - val_fmeasure: 0.3504\n",
      "Epoch 6/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.8347 - accuracy: 0.3457 - precision: 0.7728 - recall: 0.1945 - fmeasure: 0.3004src.model - INFO - {Epoch: 5} loss: 1.834691, accuracy: 0.345663, precision: 0.772837, recall: 0.194459, fmeasure: 0.300442, val_loss: 1.676800, val_accuracy: 0.409726, val_precision: 0.875786, val_recall: 0.220547, val_fmeasure: 0.344394\n",
      "191/191 [==============================] - 3s 15ms/step - loss: 1.8347 - accuracy: 0.3457 - precision: 0.7728 - recall: 0.1945 - fmeasure: 0.3004 - val_loss: 1.6768 - val_accuracy: 0.4097 - val_precision: 0.8758 - val_recall: 0.2205 - val_fmeasure: 0.3444\n",
      "Epoch 7/67\n",
      "190/191 [============================>.] - ETA: 0s - loss: 1.7870 - accuracy: 0.3688 - precision: 0.8349 - recall: 0.2125 - fmeasure: 0.3277src.model - INFO - {Epoch: 6} loss: 1.787998, accuracy: 0.367921, precision: 0.834418, recall: 0.212435, fmeasure: 0.327673, val_loss: 1.616915, val_accuracy: 0.437082, val_precision: 0.874291, val_recall: 0.253594, val_fmeasure: 0.385048\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.7880 - accuracy: 0.3679 - precision: 0.8344 - recall: 0.2124 - fmeasure: 0.3277 - val_loss: 1.6169 - val_accuracy: 0.4371 - val_precision: 0.8743 - val_recall: 0.2536 - val_fmeasure: 0.3850\n",
      "Epoch 8/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.7155 - accuracy: 0.3964 - precision: 0.8276 - recall: 0.2278 - fmeasure: 0.3476src.model - INFO - {Epoch: 7} loss: 1.715485, accuracy: 0.396399, precision: 0.827597, recall: 0.227836, fmeasure: 0.347642, val_loss: 1.512079, val_accuracy: 0.471733, val_precision: 0.937344, val_recall: 0.289535, val_fmeasure: 0.431385\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.7155 - accuracy: 0.3964 - precision: 0.8276 - recall: 0.2278 - fmeasure: 0.3476 - val_loss: 1.5121 - val_accuracy: 0.4717 - val_precision: 0.9373 - val_recall: 0.2895 - val_fmeasure: 0.4314\n",
      "Epoch 9/67\n",
      "187/191 [============================>.] - ETA: 0s - loss: 1.6315 - accuracy: 0.4308 - precision: 0.8505 - recall: 0.2620 - fmeasure: 0.3887src.model - INFO - {Epoch: 8} loss: 1.630108, accuracy: 0.431751, precision: 0.849241, recall: 0.260886, fmeasure: 0.387442, val_loss: 1.454497, val_accuracy: 0.489970, val_precision: 0.907728, val_recall: 0.321229, val_fmeasure: 0.465286\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.6301 - accuracy: 0.4318 - precision: 0.8492 - recall: 0.2609 - fmeasure: 0.3874 - val_loss: 1.4545 - val_accuracy: 0.4900 - val_precision: 0.9077 - val_recall: 0.3212 - val_fmeasure: 0.4653\n",
      "Epoch 10/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.6332 - accuracy: 0.4501 - precision: 0.8556 - recall: 0.2786 - fmeasure: 0.4101src.model - INFO - {Epoch: 9} loss: 1.633224, accuracy: 0.450082, precision: 0.855601, recall: 0.278556, fmeasure: 0.410119, val_loss: 1.481391, val_accuracy: 0.494225, val_precision: 0.896498, val_recall: 0.296817, val_fmeasure: 0.437439\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.6332 - accuracy: 0.4501 - precision: 0.8556 - recall: 0.2786 - fmeasure: 0.4101 - val_loss: 1.4814 - val_accuracy: 0.4942 - val_precision: 0.8965 - val_recall: 0.2968 - val_fmeasure: 0.4374\n",
      "Epoch 11/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.6496 - accuracy: 0.4318 - precision: 0.8420 - recall: 0.2664 - fmeasure: 0.3953src.model - INFO - {Epoch: 10} loss: 1.649556, accuracy: 0.431751, precision: 0.841960, recall: 0.266449, fmeasure: 0.395254, val_loss: 1.461903, val_accuracy: 0.510030, val_precision: 0.919317, val_recall: 0.310306, val_fmeasure: 0.453535\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.6496 - accuracy: 0.4318 - precision: 0.8420 - recall: 0.2664 - fmeasure: 0.3953 - val_loss: 1.4619 - val_accuracy: 0.5100 - val_precision: 0.9193 - val_recall: 0.3103 - val_fmeasure: 0.4535\n",
      "Epoch 12/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.5189 - accuracy: 0.4661 - precision: 0.8433 - recall: 0.3005 - fmeasure: 0.4344src.model - INFO - {Epoch: 11} loss: 1.518924, accuracy: 0.466121, precision: 0.843320, recall: 0.300545, fmeasure: 0.434379, val_loss: 1.442156, val_accuracy: 0.531307, val_precision: 0.898118, val_recall: 0.344754, val_fmeasure: 0.488061\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.5189 - accuracy: 0.4661 - precision: 0.8433 - recall: 0.3005 - fmeasure: 0.4344 - val_loss: 1.4422 - val_accuracy: 0.5313 - val_precision: 0.8981 - val_recall: 0.3448 - val_fmeasure: 0.4881\n",
      "Epoch 13/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.5084 - accuracy: 0.4887 - precision: 0.8432 - recall: 0.3270 - fmeasure: 0.4619src.model - INFO - {Epoch: 12} loss: 1.508398, accuracy: 0.488707, precision: 0.843222, recall: 0.326963, fmeasure: 0.461898, val_loss: 1.436791, val_accuracy: 0.500912, val_precision: 0.879079, val_recall: 0.326083, val_fmeasure: 0.466871\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.5084 - accuracy: 0.4887 - precision: 0.8432 - recall: 0.3270 - fmeasure: 0.4619 - val_loss: 1.4368 - val_accuracy: 0.5009 - val_precision: 0.8791 - val_recall: 0.3261 - val_fmeasure: 0.4669\n",
      "Epoch 14/67\n",
      "190/191 [============================>.] - ETA: 0s - loss: 1.4277 - accuracy: 0.5184 - precision: 0.8631 - recall: 0.3428 - fmeasure: 0.4793src.model - INFO - {Epoch: 13} loss: 1.427515, accuracy: 0.517840, precision: 0.863071, recall: 0.343063, fmeasure: 0.479619, val_loss: 1.265266, val_accuracy: 0.593921, val_precision: 0.939875, val_recall: 0.384942, val_fmeasure: 0.536824\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.4275 - accuracy: 0.5178 - precision: 0.8631 - recall: 0.3431 - fmeasure: 0.4796 - val_loss: 1.2653 - val_accuracy: 0.5939 - val_precision: 0.9399 - val_recall: 0.3849 - val_fmeasure: 0.5368\n",
      "Epoch 15/67\n",
      "189/191 [============================>.] - ETA: 0s - loss: 1.2751 - accuracy: 0.5589 - precision: 0.8762 - recall: 0.3968 - fmeasure: 0.5390src.model - INFO - {Epoch: 14} loss: 1.274420, accuracy: 0.559083, precision: 0.877489, recall: 0.396728, fmeasure: 0.539170, val_loss: 1.150383, val_accuracy: 0.623100, val_precision: 0.919291, val_recall: 0.451830, val_fmeasure: 0.598298\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.2744 - accuracy: 0.5591 - precision: 0.8775 - recall: 0.3967 - fmeasure: 0.5392 - val_loss: 1.1504 - val_accuracy: 0.6231 - val_precision: 0.9193 - val_recall: 0.4518 - val_fmeasure: 0.5983\n",
      "Epoch 16/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.2484 - accuracy: 0.5820 - precision: 0.8689 - recall: 0.4258 - fmeasure: 0.5626src.model - INFO - {Epoch: 15} loss: 1.248416, accuracy: 0.581997, precision: 0.868898, recall: 0.425829, fmeasure: 0.562574, val_loss: 1.063689, val_accuracy: 0.651672, val_precision: 0.940101, val_recall: 0.495519, val_fmeasure: 0.640534\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.2484 - accuracy: 0.5820 - precision: 0.8689 - recall: 0.4258 - fmeasure: 0.5626 - val_loss: 1.0637 - val_accuracy: 0.6517 - val_precision: 0.9401 - val_recall: 0.4955 - val_fmeasure: 0.6405\n",
      "Epoch 17/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.1168 - accuracy: 0.6196 - precision: 0.8906 - recall: 0.4720 - fmeasure: 0.6091src.model - INFO - {Epoch: 16} loss: 1.116813, accuracy: 0.619640, precision: 0.890634, recall: 0.472033, fmeasure: 0.609147, val_loss: 0.962017, val_accuracy: 0.692401, val_precision: 0.937188, val_recall: 0.531320, val_fmeasure: 0.671307\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.1168 - accuracy: 0.6196 - precision: 0.8906 - recall: 0.4720 - fmeasure: 0.6091 - val_loss: 0.9620 - val_accuracy: 0.6924 - val_precision: 0.9372 - val_recall: 0.5313 - val_fmeasure: 0.6713\n",
      "Epoch 18/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.9516 - accuracy: 0.6740 - precision: 0.8969 - recall: 0.5403 - fmeasure: 0.6671src.model - INFO - {Epoch: 17} loss: 0.951571, accuracy: 0.673977, precision: 0.896857, recall: 0.540336, fmeasure: 0.667127, val_loss: 1.020890, val_accuracy: 0.677812, val_precision: 0.894771, val_recall: 0.581824, val_fmeasure: 0.699484\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.9516 - accuracy: 0.6740 - precision: 0.8969 - recall: 0.5403 - fmeasure: 0.6671 - val_loss: 1.0209 - val_accuracy: 0.6778 - val_precision: 0.8948 - val_recall: 0.5818 - val_fmeasure: 0.6995\n",
      "Epoch 19/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.2540 - accuracy: 0.6245 - precision: 0.8363 - recall: 0.4740 - fmeasure: 0.5966src.model - INFO - {Epoch: 18} loss: 1.253983, accuracy: 0.624550, precision: 0.836253, recall: 0.473953, fmeasure: 0.596612, val_loss: 0.927055, val_accuracy: 0.705775, val_precision: 0.925802, val_recall: 0.542849, val_fmeasure: 0.677914\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.2540 - accuracy: 0.6245 - precision: 0.8363 - recall: 0.4740 - fmeasure: 0.5966 - val_loss: 0.9271 - val_accuracy: 0.7058 - val_precision: 0.9258 - val_recall: 0.5428 - val_fmeasure: 0.6779\n",
      "Epoch 20/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.9989 - accuracy: 0.6717 - precision: 0.8764 - recall: 0.5414 - fmeasure: 0.6637src.model - INFO - {Epoch: 19} loss: 0.998933, accuracy: 0.671686, precision: 0.876374, recall: 0.541361, fmeasure: 0.663676, val_loss: 0.852591, val_accuracy: 0.718541, val_precision: 0.931613, val_recall: 0.589712, val_fmeasure: 0.716917\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.9989 - accuracy: 0.6717 - precision: 0.8764 - recall: 0.5414 - fmeasure: 0.6637 - val_loss: 0.8526 - val_accuracy: 0.7185 - val_precision: 0.9316 - val_recall: 0.5897 - val_fmeasure: 0.7169\n",
      "Epoch 21/67\n",
      "188/191 [============================>.] - ETA: 0s - loss: 0.9572 - accuracy: 0.6732 - precision: 0.8747 - recall: 0.5602 - fmeasure: 0.6778src.model - INFO - {Epoch: 20} loss: 0.958484, accuracy: 0.672995, precision: 0.874426, recall: 0.559097, fmeasure: 0.676903, val_loss: 0.874550, val_accuracy: 0.743465, val_precision: 0.924683, val_recall: 0.590459, val_fmeasure: 0.714423\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.9585 - accuracy: 0.6730 - precision: 0.8744 - recall: 0.5591 - fmeasure: 0.6769 - val_loss: 0.8745 - val_accuracy: 0.7435 - val_precision: 0.9247 - val_recall: 0.5905 - val_fmeasure: 0.7144\n",
      "Epoch 22/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.8818 - accuracy: 0.7093 - precision: 0.8937 - recall: 0.5901 - fmeasure: 0.7055src.model - INFO - {Epoch: 21} loss: 0.881792, accuracy: 0.709329, precision: 0.893678, recall: 0.590118, fmeasure: 0.705542, val_loss: 0.834666, val_accuracy: 0.763526, val_precision: 0.922709, val_recall: 0.605349, val_fmeasure: 0.725506\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.8818 - accuracy: 0.7093 - precision: 0.8937 - recall: 0.5901 - fmeasure: 0.7055 - val_loss: 0.8347 - val_accuracy: 0.7635 - val_precision: 0.9227 - val_recall: 0.6053 - val_fmeasure: 0.7255\n",
      "Epoch 23/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7767 - accuracy: 0.7381 - precision: 0.8995 - recall: 0.6259 - fmeasure: 0.7326src.model - INFO - {Epoch: 22} loss: 0.776716, accuracy: 0.738134, precision: 0.899466, recall: 0.625851, fmeasure: 0.732622, val_loss: 0.705659, val_accuracy: 0.797568, val_precision: 0.935215, val_recall: 0.665095, val_fmeasure: 0.773822\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.7767 - accuracy: 0.7381 - precision: 0.8995 - recall: 0.6259 - fmeasure: 0.7326 - val_loss: 0.7057 - val_accuracy: 0.7976 - val_precision: 0.9352 - val_recall: 0.6651 - val_fmeasure: 0.7738\n",
      "Epoch 24/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7080 - accuracy: 0.7679 - precision: 0.9020 - recall: 0.6576 - fmeasure: 0.7557src.model - INFO - {Epoch: 23} loss: 0.708014, accuracy: 0.767921, precision: 0.901964, recall: 0.657592, fmeasure: 0.755659, val_loss: 0.782652, val_accuracy: 0.798176, val_precision: 0.905706, val_recall: 0.675551, val_fmeasure: 0.769712\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.7080 - accuracy: 0.7679 - precision: 0.9020 - recall: 0.6576 - fmeasure: 0.7557 - val_loss: 0.7827 - val_accuracy: 0.7982 - val_precision: 0.9057 - val_recall: 0.6756 - val_fmeasure: 0.7697\n",
      "Epoch 25/67\n",
      "188/191 [============================>.] - ETA: 0s - loss: 0.8905 - accuracy: 0.7324 - precision: 0.8676 - recall: 0.6233 - fmeasure: 0.7215src.model - INFO - {Epoch: 24} loss: 0.895480, accuracy: 0.732242, precision: 0.867066, recall: 0.623277, fmeasure: 0.721251, val_loss: 0.807665, val_accuracy: 0.778723, val_precision: 0.905040, val_recall: 0.640077, val_fmeasure: 0.745779\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.8955 - accuracy: 0.7322 - precision: 0.8671 - recall: 0.6233 - fmeasure: 0.7213 - val_loss: 0.8077 - val_accuracy: 0.7787 - val_precision: 0.9050 - val_recall: 0.6401 - val_fmeasure: 0.7458\n",
      "Epoch 26/67\n",
      "188/191 [============================>.] - ETA: 0s - loss: 1.0641 - accuracy: 0.7171 - precision: 0.8578 - recall: 0.6021 - fmeasure: 0.7022src.model - INFO - {Epoch: 25} loss: 1.056751, accuracy: 0.718167, precision: 0.859186, recall: 0.604298, fmeasure: 0.704220, val_loss: 0.784497, val_accuracy: 0.792097, val_precision: 0.900970, val_recall: 0.637043, val_fmeasure: 0.741311\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.0568 - accuracy: 0.7182 - precision: 0.8592 - recall: 0.6043 - fmeasure: 0.7042 - val_loss: 0.7845 - val_accuracy: 0.7921 - val_precision: 0.9010 - val_recall: 0.6370 - val_fmeasure: 0.7413\n",
      "Epoch 27/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7390 - accuracy: 0.7738 - precision: 0.8945 - recall: 0.6681 - fmeasure: 0.7604src.model - INFO - {Epoch: 26} loss: 0.739041, accuracy: 0.773813, precision: 0.894456, recall: 0.668063, fmeasure: 0.760434, val_loss: 0.650635, val_accuracy: 0.813374, val_precision: 0.926497, val_recall: 0.705891, val_fmeasure: 0.797291\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.7390 - accuracy: 0.7738 - precision: 0.8945 - recall: 0.6681 - fmeasure: 0.7604 - val_loss: 0.6506 - val_accuracy: 0.8134 - val_precision: 0.9265 - val_recall: 0.7059 - val_fmeasure: 0.7973\n",
      "Epoch 28/67\n",
      "187/191 [============================>.] - ETA: 0s - loss: 0.7128 - accuracy: 0.7834 - precision: 0.8808 - recall: 0.6775 - fmeasure: 0.7623src.model - INFO - {Epoch: 27} loss: 0.711458, accuracy: 0.782651, precision: 0.881367, recall: 0.677181, fmeasure: 0.762266, val_loss: 0.665081, val_accuracy: 0.813374, val_precision: 0.909313, val_recall: 0.731236, val_fmeasure: 0.807778\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.7115 - accuracy: 0.7827 - precision: 0.8814 - recall: 0.6772 - fmeasure: 0.7623 - val_loss: 0.6651 - val_accuracy: 0.8134 - val_precision: 0.9093 - val_recall: 0.7312 - val_fmeasure: 0.8078\n",
      "Epoch 29/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.5957 - accuracy: 0.8095 - precision: 0.9004 - recall: 0.7185 - fmeasure: 0.7962src.model - INFO - {Epoch: 28} loss: 0.595716, accuracy: 0.809493, precision: 0.900374, recall: 0.718477, fmeasure: 0.796190, val_loss: 0.528521, val_accuracy: 0.838906, val_precision: 0.908201, val_recall: 0.768857, val_fmeasure: 0.830250\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.5957 - accuracy: 0.8095 - precision: 0.9004 - recall: 0.7185 - fmeasure: 0.7962 - val_loss: 0.5285 - val_accuracy: 0.8389 - val_precision: 0.9082 - val_recall: 0.7689 - val_fmeasure: 0.8302\n",
      "Epoch 30/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.8252 - precision: 0.8991 - recall: 0.7496 - fmeasure: 0.8149src.model - INFO - {Epoch: 29} loss: 0.524825, accuracy: 0.825205, precision: 0.899109, recall: 0.749586, fmeasure: 0.814892, val_loss: 0.499828, val_accuracy: 0.844985, val_precision: 0.918276, val_recall: 0.773712, val_fmeasure: 0.837155\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.5248 - accuracy: 0.8252 - precision: 0.8991 - recall: 0.7496 - fmeasure: 0.8149 - val_loss: 0.4998 - val_accuracy: 0.8450 - val_precision: 0.9183 - val_recall: 0.7737 - val_fmeasure: 0.8372\n",
      "Epoch 31/67\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.4591 - accuracy: 0.8447 - precision: 0.9141 - recall: 0.7766 - fmeasure: 0.8374src.model - INFO - {Epoch: 30} loss: 0.459374, accuracy: 0.844845, precision: 0.914583, recall: 0.776418, fmeasure: 0.837483, val_loss: 0.475657, val_accuracy: 0.864438, val_precision: 0.918408, val_recall: 0.805545, val_fmeasure: 0.856649\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.4594 - accuracy: 0.8448 - precision: 0.9146 - recall: 0.7764 - fmeasure: 0.8375 - val_loss: 0.4757 - val_accuracy: 0.8644 - val_precision: 0.9184 - val_recall: 0.8055 - val_fmeasure: 0.8566\n",
      "Epoch 32/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.8435 - precision: 0.9040 - recall: 0.7839 - fmeasure: 0.8376src.model - INFO - {Epoch: 31} loss: 0.474510, accuracy: 0.843535, precision: 0.904021, recall: 0.783922, fmeasure: 0.837582, val_loss: 0.525931, val_accuracy: 0.843161, val_precision: 0.901451, val_recall: 0.786595, val_fmeasure: 0.838029\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.4745 - accuracy: 0.8435 - precision: 0.9040 - recall: 0.7839 - fmeasure: 0.8376 - val_loss: 0.5259 - val_accuracy: 0.8432 - val_precision: 0.9015 - val_recall: 0.7866 - val_fmeasure: 0.8380\n",
      "Epoch 33/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.8579 - precision: 0.9058 - recall: 0.7974 - fmeasure: 0.8460src.model - INFO - {Epoch: 32} loss: 0.425497, accuracy: 0.857938, precision: 0.905845, recall: 0.797404, fmeasure: 0.845981, val_loss: 0.449345, val_accuracy: 0.863830, val_precision: 0.906236, val_recall: 0.825429, val_fmeasure: 0.862687\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.4255 - accuracy: 0.8579 - precision: 0.9058 - recall: 0.7974 - fmeasure: 0.8460 - val_loss: 0.4493 - val_accuracy: 0.8638 - val_precision: 0.9062 - val_recall: 0.8254 - val_fmeasure: 0.8627\n",
      "Epoch 34/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.8596 - precision: 0.9123 - recall: 0.8082 - fmeasure: 0.8553src.model - INFO - {Epoch: 33} loss: 0.433519, accuracy: 0.859574, precision: 0.912317, recall: 0.808181, fmeasure: 0.855260, val_loss: 0.460753, val_accuracy: 0.860182, val_precision: 0.917446, val_recall: 0.812687, val_fmeasure: 0.859818\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.4335 - accuracy: 0.8596 - precision: 0.9123 - recall: 0.8082 - fmeasure: 0.8553 - val_loss: 0.4608 - val_accuracy: 0.8602 - val_precision: 0.9174 - val_recall: 0.8127 - val_fmeasure: 0.8598\n",
      "Epoch 35/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.8717 - precision: 0.9115 - recall: 0.8242 - fmeasure: 0.8641src.model - INFO - {Epoch: 34} loss: 0.369468, accuracy: 0.871686, precision: 0.911529, recall: 0.824215, fmeasure: 0.864109, val_loss: 0.425763, val_accuracy: 0.880851, val_precision: 0.918127, val_recall: 0.857730, val_fmeasure: 0.885907\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3695 - accuracy: 0.8717 - precision: 0.9115 - recall: 0.8242 - fmeasure: 0.8641 - val_loss: 0.4258 - val_accuracy: 0.8809 - val_precision: 0.9181 - val_recall: 0.8577 - val_fmeasure: 0.8859\n",
      "Epoch 36/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.8838 - precision: 0.9192 - recall: 0.8481 - fmeasure: 0.8813src.model - INFO - {Epoch: 35} loss: 0.327808, accuracy: 0.883797, precision: 0.919225, recall: 0.848124, fmeasure: 0.881260, val_loss: 0.546090, val_accuracy: 0.872340, val_precision: 0.904235, val_recall: 0.836632, val_fmeasure: 0.868006\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3278 - accuracy: 0.8838 - precision: 0.9192 - recall: 0.8481 - fmeasure: 0.8813 - val_loss: 0.5461 - val_accuracy: 0.8723 - val_precision: 0.9042 - val_recall: 0.8366 - val_fmeasure: 0.8680\n",
      "Epoch 37/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8609 - precision: 0.9098 - recall: 0.8258 - fmeasure: 0.8638src.model - INFO - {Epoch: 36} loss: 0.420424, accuracy: 0.860884, precision: 0.909785, recall: 0.825829, fmeasure: 0.863756, val_loss: 0.481456, val_accuracy: 0.860182, val_precision: 0.914686, val_recall: 0.792802, val_fmeasure: 0.847041\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.4204 - accuracy: 0.8609 - precision: 0.9098 - recall: 0.8258 - fmeasure: 0.8638 - val_loss: 0.4815 - val_accuracy: 0.8602 - val_precision: 0.9147 - val_recall: 0.7928 - val_fmeasure: 0.8470\n",
      "Epoch 38/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3935 - accuracy: 0.8792 - precision: 0.9152 - recall: 0.8428 - fmeasure: 0.8764src.model - INFO - {Epoch: 37} loss: 0.393526, accuracy: 0.879214, precision: 0.915238, recall: 0.842845, fmeasure: 0.876401, val_loss: 0.650778, val_accuracy: 0.869909, val_precision: 0.912711, val_recall: 0.827857, val_fmeasure: 0.866856\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3935 - accuracy: 0.8792 - precision: 0.9152 - recall: 0.8428 - fmeasure: 0.8764 - val_loss: 0.6508 - val_accuracy: 0.8699 - val_precision: 0.9127 - val_recall: 0.8279 - val_fmeasure: 0.8669\n",
      "Epoch 39/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.3277 - accuracy: 0.7568 - precision: 0.8189 - recall: 0.6888 - fmeasure: 0.7451src.model - INFO - {Epoch: 38} loss: 1.327706, accuracy: 0.756792, precision: 0.818878, recall: 0.688765, fmeasure: 0.745065, val_loss: 0.535431, val_accuracy: 0.849240, val_precision: 0.904749, val_recall: 0.807226, val_fmeasure: 0.851651\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 1.3277 - accuracy: 0.7568 - precision: 0.8189 - recall: 0.6888 - fmeasure: 0.7451 - val_loss: 0.5354 - val_accuracy: 0.8492 - val_precision: 0.9047 - val_recall: 0.8072 - val_fmeasure: 0.8517\n",
      "Epoch 40/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.5312 - accuracy: 0.8488 - precision: 0.8938 - recall: 0.8000 - fmeasure: 0.8426src.model - INFO - {Epoch: 39} loss: 0.531167, accuracy: 0.848773, precision: 0.893805, recall: 0.800000, fmeasure: 0.842619, val_loss: 0.432210, val_accuracy: 0.873556, val_precision: 0.904407, val_recall: 0.853949, val_fmeasure: 0.877585\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.5312 - accuracy: 0.8488 - precision: 0.8938 - recall: 0.8000 - fmeasure: 0.8426 - val_loss: 0.4322 - val_accuracy: 0.8736 - val_precision: 0.9044 - val_recall: 0.8539 - val_fmeasure: 0.8776\n",
      "Epoch 41/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.8655 - precision: 0.9012 - recall: 0.8318 - fmeasure: 0.8638src.model - INFO - {Epoch: 40} loss: 0.476214, accuracy: 0.865466, precision: 0.901191, recall: 0.831763, fmeasure: 0.863837, val_loss: 0.446664, val_accuracy: 0.893009, val_precision: 0.918103, val_recall: 0.872900, val_fmeasure: 0.894090\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.4762 - accuracy: 0.8655 - precision: 0.9012 - recall: 0.8318 - fmeasure: 0.8638 - val_loss: 0.4467 - val_accuracy: 0.8930 - val_precision: 0.9181 - val_recall: 0.8729 - val_fmeasure: 0.8941\n",
      "Epoch 42/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.8773 - precision: 0.9086 - recall: 0.8475 - fmeasure: 0.8761src.model - INFO - {Epoch: 41} loss: 0.396819, accuracy: 0.877250, precision: 0.908642, recall: 0.847469, fmeasure: 0.876125, val_loss: 0.411232, val_accuracy: 0.870517, val_precision: 0.905514, val_recall: 0.854556, val_fmeasure: 0.878604\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3968 - accuracy: 0.8773 - precision: 0.9086 - recall: 0.8475 - fmeasure: 0.8761 - val_loss: 0.4112 - val_accuracy: 0.8705 - val_precision: 0.9055 - val_recall: 0.8546 - val_fmeasure: 0.8786\n",
      "Epoch 43/67\n",
      "187/191 [============================>.] - ETA: 0s - loss: 0.3394 - accuracy: 0.8880 - precision: 0.9147 - recall: 0.8559 - fmeasure: 0.8833src.model - INFO - {Epoch: 42} loss: 0.337657, accuracy: 0.887725, precision: 0.914769, recall: 0.855999, fmeasure: 0.883354, val_loss: 0.487071, val_accuracy: 0.888146, val_precision: 0.905362, val_recall: 0.874860, val_fmeasure: 0.889368\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3377 - accuracy: 0.8877 - precision: 0.9148 - recall: 0.8560 - fmeasure: 0.8834 - val_loss: 0.4871 - val_accuracy: 0.8881 - val_precision: 0.9054 - val_recall: 0.8749 - val_fmeasure: 0.8894\n",
      "Epoch 44/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3149 - accuracy: 0.9028 - precision: 0.9317 - recall: 0.8779 - fmeasure: 0.9033src.model - INFO - {Epoch: 43} loss: 0.314929, accuracy: 0.902782, precision: 0.931748, recall: 0.877901, fmeasure: 0.903268, val_loss: 0.544086, val_accuracy: 0.884498, val_precision: 0.906185, val_recall: 0.870472, val_fmeasure: 0.887408\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3149 - accuracy: 0.9028 - precision: 0.9317 - recall: 0.8779 - fmeasure: 0.9033 - val_loss: 0.5441 - val_accuracy: 0.8845 - val_precision: 0.9062 - val_recall: 0.8705 - val_fmeasure: 0.8874\n",
      "Epoch 45/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4661 - accuracy: 0.8743 - precision: 0.9054 - recall: 0.8455 - fmeasure: 0.8732src.model - INFO - {Epoch: 44} loss: 0.466051, accuracy: 0.874304, precision: 0.905390, recall: 0.845484, fmeasure: 0.873246, val_loss: 0.468462, val_accuracy: 0.893009, val_precision: 0.920006, val_recall: 0.871686, val_fmeasure: 0.894412\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.4661 - accuracy: 0.8743 - precision: 0.9054 - recall: 0.8455 - fmeasure: 0.8732 - val_loss: 0.4685 - val_accuracy: 0.8930 - val_precision: 0.9200 - val_recall: 0.8717 - val_fmeasure: 0.8944\n",
      "Epoch 46/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3083 - accuracy: 0.8979 - precision: 0.9210 - recall: 0.8719 - fmeasure: 0.8950src.model - INFO - {Epoch: 45} loss: 0.308327, accuracy: 0.897872, precision: 0.921014, recall: 0.871946, fmeasure: 0.894983, val_loss: 0.436803, val_accuracy: 0.900304, val_precision: 0.920935, val_recall: 0.890637, val_fmeasure: 0.905108\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3083 - accuracy: 0.8979 - precision: 0.9210 - recall: 0.8719 - fmeasure: 0.8950 - val_loss: 0.4368 - val_accuracy: 0.9003 - val_precision: 0.9209 - val_recall: 0.8906 - val_fmeasure: 0.9051\n",
      "Epoch 47/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2694 - accuracy: 0.9103 - precision: 0.9327 - recall: 0.8858 - fmeasure: 0.9079src.model - INFO - {Epoch: 46} loss: 0.269355, accuracy: 0.910311, precision: 0.932744, recall: 0.885798, fmeasure: 0.907862, val_loss: 0.434773, val_accuracy: 0.903951, val_precision: 0.925938, val_recall: 0.889890, val_fmeasure: 0.907090\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.2694 - accuracy: 0.9103 - precision: 0.9327 - recall: 0.8858 - fmeasure: 0.9079 - val_loss: 0.4348 - val_accuracy: 0.9040 - val_precision: 0.9259 - val_recall: 0.8899 - val_fmeasure: 0.9071\n",
      "Epoch 48/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.9070 - precision: 0.9254 - recall: 0.8864 - fmeasure: 0.9049src.model - INFO - {Epoch: 47} loss: 0.340528, accuracy: 0.907038, precision: 0.925376, recall: 0.886431, fmeasure: 0.904853, val_loss: 0.390331, val_accuracy: 0.910030, val_precision: 0.927384, val_recall: 0.902633, val_fmeasure: 0.914538\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3405 - accuracy: 0.9070 - precision: 0.9254 - recall: 0.8864 - fmeasure: 0.9049 - val_loss: 0.3903 - val_accuracy: 0.9100 - val_precision: 0.9274 - val_recall: 0.9026 - val_fmeasure: 0.9145\n",
      "Epoch 49/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.9149 - precision: 0.9339 - recall: 0.8930 - fmeasure: 0.9122src.model - INFO - {Epoch: 48} loss: 0.293033, accuracy: 0.914894, precision: 0.933889, recall: 0.892954, fmeasure: 0.912207, val_loss: 0.568037, val_accuracy: 0.907599, val_precision: 0.927552, val_recall: 0.886996, val_fmeasure: 0.906247\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.2930 - accuracy: 0.9149 - precision: 0.9339 - recall: 0.8930 - fmeasure: 0.9122 - val_loss: 0.5680 - val_accuracy: 0.9076 - val_precision: 0.9276 - val_recall: 0.8870 - val_fmeasure: 0.9062\n",
      "Epoch 50/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.9133 - precision: 0.9316 - recall: 0.8933 - fmeasure: 0.9114src.model - INFO - {Epoch: 49} loss: 0.380957, accuracy: 0.913257, precision: 0.931593, recall: 0.893325, fmeasure: 0.911405, val_loss: 0.565449, val_accuracy: 0.908207, val_precision: 0.924009, val_recall: 0.900205, val_fmeasure: 0.911604\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3810 - accuracy: 0.9133 - precision: 0.9316 - recall: 0.8933 - fmeasure: 0.9114 - val_loss: 0.5654 - val_accuracy: 0.9082 - val_precision: 0.9240 - val_recall: 0.9002 - val_fmeasure: 0.9116\n",
      "Epoch 51/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.9123 - precision: 0.9309 - recall: 0.8946 - fmeasure: 0.9118src.model - INFO - {Epoch: 50} loss: 0.313539, accuracy: 0.912275, precision: 0.930889, recall: 0.894634, fmeasure: 0.911775, val_loss: 0.424391, val_accuracy: 0.904559, val_precision: 0.921518, val_recall: 0.889890, val_fmeasure: 0.904838\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3135 - accuracy: 0.9123 - precision: 0.9309 - recall: 0.8946 - fmeasure: 0.9118 - val_loss: 0.4244 - val_accuracy: 0.9046 - val_precision: 0.9215 - val_recall: 0.8899 - val_fmeasure: 0.9048\n",
      "Epoch 52/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4916 - accuracy: 0.8900 - precision: 0.9095 - recall: 0.8694 - fmeasure: 0.8882src.model - INFO - {Epoch: 51} loss: 0.491592, accuracy: 0.890016, precision: 0.909457, recall: 0.869415, fmeasure: 0.888249, val_loss: 0.578396, val_accuracy: 0.893617, val_precision: 0.918227, val_recall: 0.873646, val_fmeasure: 0.894610\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.4916 - accuracy: 0.8900 - precision: 0.9095 - recall: 0.8694 - fmeasure: 0.8882 - val_loss: 0.5784 - val_accuracy: 0.8936 - val_precision: 0.9182 - val_recall: 0.8736 - val_fmeasure: 0.8946\n",
      "Epoch 53/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3889 - accuracy: 0.8975 - precision: 0.9175 - recall: 0.8720 - fmeasure: 0.8933src.model - INFO - {Epoch: 52} loss: 0.388933, accuracy: 0.897545, precision: 0.917490, recall: 0.872033, fmeasure: 0.893346, val_loss: 0.543365, val_accuracy: 0.910030, val_precision: 0.925506, val_recall: 0.905667, val_fmeasure: 0.915189\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3889 - accuracy: 0.8975 - precision: 0.9175 - recall: 0.8720 - fmeasure: 0.8933 - val_loss: 0.5434 - val_accuracy: 0.9100 - val_precision: 0.9255 - val_recall: 0.9057 - val_fmeasure: 0.9152\n",
      "Epoch 54/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3134 - accuracy: 0.9198 - precision: 0.9362 - recall: 0.9048 - fmeasure: 0.9197src.model - INFO - {Epoch: 53} loss: 0.313415, accuracy: 0.919804, precision: 0.936186, recall: 0.904756, fmeasure: 0.919740, val_loss: 0.422073, val_accuracy: 0.906991, val_precision: 0.920221, val_recall: 0.900205, val_fmeasure: 0.909783\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3134 - accuracy: 0.9198 - precision: 0.9362 - recall: 0.9048 - fmeasure: 0.9197 - val_loss: 0.4221 - val_accuracy: 0.9070 - val_precision: 0.9202 - val_recall: 0.9002 - val_fmeasure: 0.9098\n",
      "Epoch 55/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.9178 - precision: 0.9349 - recall: 0.9008 - fmeasure: 0.9170src.model - INFO - {Epoch: 54} loss: 0.286789, accuracy: 0.917840, precision: 0.934856, recall: 0.900807, fmeasure: 0.917045, val_loss: 0.432864, val_accuracy: 0.914894, val_precision: 0.927072, val_recall: 0.906880, val_fmeasure: 0.916576\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.2868 - accuracy: 0.9178 - precision: 0.9349 - recall: 0.9008 - fmeasure: 0.9170 - val_loss: 0.4329 - val_accuracy: 0.9149 - val_precision: 0.9271 - val_recall: 0.9069 - val_fmeasure: 0.9166\n",
      "Epoch 56/67\n",
      "189/191 [============================>.] - ETA: 0s - loss: 0.4344 - accuracy: 0.9094 - precision: 0.9304 - recall: 0.8882 - fmeasure: 0.9080src.model - INFO - {Epoch: 55} loss: 0.432735, accuracy: 0.909656, precision: 0.930746, recall: 0.887369, fmeasure: 0.907695, val_loss: 0.690064, val_accuracy: 0.878419, val_precision: 0.893824, val_recall: 0.867438, val_fmeasure: 0.880055\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.4327 - accuracy: 0.9097 - precision: 0.9307 - recall: 0.8874 - fmeasure: 0.9077 - val_loss: 0.6901 - val_accuracy: 0.8784 - val_precision: 0.8938 - val_recall: 0.8674 - val_fmeasure: 0.8801\n",
      "Epoch 57/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.9008 - precision: 0.9174 - recall: 0.8821 - fmeasure: 0.8988src.model - INFO - {Epoch: 56} loss: 0.387642, accuracy: 0.900818, precision: 0.917398, recall: 0.882134, fmeasure: 0.898800, val_loss: 0.417013, val_accuracy: 0.900912, val_precision: 0.917797, val_recall: 0.893064, val_fmeasure: 0.904837\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3876 - accuracy: 0.9008 - precision: 0.9174 - recall: 0.8821 - fmeasure: 0.8988 - val_loss: 0.4170 - val_accuracy: 0.9009 - val_precision: 0.9178 - val_recall: 0.8931 - val_fmeasure: 0.9048\n",
      "Epoch 58/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.9237 - precision: 0.9375 - recall: 0.9067 - fmeasure: 0.9213src.model - INFO - {Epoch: 57} loss: 0.249202, accuracy: 0.923732, precision: 0.937480, recall: 0.906719, fmeasure: 0.921275, val_loss: 0.513432, val_accuracy: 0.893617, val_precision: 0.902318, val_recall: 0.889143, val_fmeasure: 0.895384\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.2492 - accuracy: 0.9237 - precision: 0.9375 - recall: 0.9067 - fmeasure: 0.9213 - val_loss: 0.5134 - val_accuracy: 0.8936 - val_precision: 0.9023 - val_recall: 0.8891 - val_fmeasure: 0.8954\n",
      "Epoch 59/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9365 - precision: 0.9498 - recall: 0.9241 - fmeasure: 0.9364src.model - INFO - {Epoch: 58} loss: 0.188688, accuracy: 0.936498, precision: 0.949844, recall: 0.924084, fmeasure: 0.936386, val_loss: 0.419383, val_accuracy: 0.922188, val_precision: 0.939104, val_recall: 0.914909, val_fmeasure: 0.926369\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.1887 - accuracy: 0.9365 - precision: 0.9498 - recall: 0.9241 - fmeasure: 0.9364 - val_loss: 0.4194 - val_accuracy: 0.9222 - val_precision: 0.9391 - val_recall: 0.9149 - val_fmeasure: 0.9264\n",
      "Epoch 60/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9375 - precision: 0.9495 - recall: 0.9273 - fmeasure: 0.9380src.model - INFO - {Epoch: 59} loss: 0.192578, accuracy: 0.937480, precision: 0.949539, recall: 0.927334, fmeasure: 0.937989, val_loss: 0.550977, val_accuracy: 0.903951, val_precision: 0.918721, val_recall: 0.891850, val_fmeasure: 0.904681\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.1926 - accuracy: 0.9375 - precision: 0.9495 - recall: 0.9273 - fmeasure: 0.9380 - val_loss: 0.5510 - val_accuracy: 0.9040 - val_precision: 0.9187 - val_recall: 0.8919 - val_fmeasure: 0.9047\n",
      "Epoch 61/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9264 - precision: 0.9388 - recall: 0.9162 - fmeasure: 0.9270src.model - INFO - {Epoch: 60} loss: 0.241073, accuracy: 0.926350, precision: 0.938840, recall: 0.916187, fmeasure: 0.927010, val_loss: 0.434967, val_accuracy: 0.920973, val_precision: 0.939462, val_recall: 0.911128, val_fmeasure: 0.924623\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.2411 - accuracy: 0.9264 - precision: 0.9388 - recall: 0.9162 - fmeasure: 0.9270 - val_loss: 0.4350 - val_accuracy: 0.9210 - val_precision: 0.9395 - val_recall: 0.9111 - val_fmeasure: 0.9246\n",
      "Epoch 62/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.9378 - precision: 0.9493 - recall: 0.9240 - fmeasure: 0.9361src.model - INFO - {Epoch: 61} loss: 0.242970, accuracy: 0.937807, precision: 0.949346, recall: 0.924040, fmeasure: 0.936086, val_loss: 0.513221, val_accuracy: 0.906383, val_precision: 0.919926, val_recall: 0.896565, val_fmeasure: 0.907764\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.2430 - accuracy: 0.9378 - precision: 0.9493 - recall: 0.9240 - fmeasure: 0.9361 - val_loss: 0.5132 - val_accuracy: 0.9064 - val_precision: 0.9199 - val_recall: 0.8966 - val_fmeasure: 0.9078\n",
      "Epoch 63/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.9309 - precision: 0.9452 - recall: 0.9228 - fmeasure: 0.9335src.model - INFO - {Epoch: 62} loss: 0.282395, accuracy: 0.930933, precision: 0.945160, recall: 0.922753, fmeasure: 0.933488, val_loss: 0.588423, val_accuracy: 0.911246, val_precision: 0.926443, val_recall: 0.900952, val_fmeasure: 0.913066\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.2824 - accuracy: 0.9309 - precision: 0.9452 - recall: 0.9228 - fmeasure: 0.9335 - val_loss: 0.5884 - val_accuracy: 0.9112 - val_precision: 0.9264 - val_recall: 0.9010 - val_fmeasure: 0.9131\n",
      "Epoch 64/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.5225 - accuracy: 0.8822 - precision: 0.9087 - recall: 0.8638 - fmeasure: 0.8848src.model - INFO - {Epoch: 63} loss: 0.522481, accuracy: 0.882160, precision: 0.908750, recall: 0.863809, fmeasure: 0.884822, val_loss: 0.800393, val_accuracy: 0.868693, val_precision: 0.898753, val_recall: 0.850588, val_fmeasure: 0.873151\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.5225 - accuracy: 0.8822 - precision: 0.9087 - recall: 0.8638 - fmeasure: 0.8848 - val_loss: 0.8004 - val_accuracy: 0.8687 - val_precision: 0.8988 - val_recall: 0.8506 - val_fmeasure: 0.8732\n",
      "Epoch 65/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.9002 - precision: 0.9240 - recall: 0.8805 - fmeasure: 0.9011src.model - INFO - {Epoch: 64} loss: 0.322834, accuracy: 0.900164, precision: 0.923983, recall: 0.880541, fmeasure: 0.901050, val_loss: 0.585099, val_accuracy: 0.912462, val_precision: 0.930320, val_recall: 0.905060, val_fmeasure: 0.917119\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3228 - accuracy: 0.9002 - precision: 0.9240 - recall: 0.8805 - fmeasure: 0.9011 - val_loss: 0.5851 - val_accuracy: 0.9125 - val_precision: 0.9303 - val_recall: 0.9051 - val_fmeasure: 0.9171\n",
      "Epoch 66/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.9162 - precision: 0.9302 - recall: 0.9041 - fmeasure: 0.9166src.model - INFO - {Epoch: 65} loss: 0.294821, accuracy: 0.916203, precision: 0.930205, recall: 0.904058, fmeasure: 0.916575, val_loss: 0.689533, val_accuracy: 0.911246, val_precision: 0.929394, val_recall: 0.897171, val_fmeasure: 0.912552\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.2948 - accuracy: 0.9162 - precision: 0.9302 - recall: 0.9041 - fmeasure: 0.9166 - val_loss: 0.6895 - val_accuracy: 0.9112 - val_precision: 0.9294 - val_recall: 0.8972 - val_fmeasure: 0.9126\n",
      "Epoch 67/67\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3322 - accuracy: 0.9136 - precision: 0.9293 - recall: 0.8950 - fmeasure: 0.9112src.model - INFO - {Epoch: 66} loss: 0.332198, accuracy: 0.913584, precision: 0.929349, recall: 0.894961, fmeasure: 0.911240, val_loss: 0.458397, val_accuracy: 0.911246, val_precision: 0.932827, val_recall: 0.898245, val_fmeasure: 0.914584\n",
      "191/191 [==============================] - 3s 14ms/step - loss: 0.3322 - accuracy: 0.9136 - precision: 0.9293 - recall: 0.8950 - fmeasure: 0.9112 - val_loss: 0.4584 - val_accuracy: 0.9112 - val_precision: 0.9328 - val_recall: 0.8982 - val_fmeasure: 0.9146\n",
      "src.model - INFO - Training completed\n",
      "src.model - INFO - Evaluating model\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4584 - accuracy: 0.9112 - precision: 0.9310 - recall: 0.8970 - fmeasure: 0.9135\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0733 - accuracy: 0.9738 - precision: 0.9808 - recall: 0.9680 - fmeasure: 0.9743\n",
      "src.model - INFO - Train loss: 0.07329725474119186\n",
      "src.model - INFO - Train precision: 0.9807999730110168\n",
      "src.model - INFO - Train recall: 0.9680121541023254\n",
      "src.model - INFO - Train f1-score: 0.9742832779884338\n",
      "src.model - INFO - Test loss: 0.458377480506897\n",
      "src.model - INFO - Test precision: 0.9310498833656311\n",
      "src.model - INFO - Test recall: 0.8970044255256653\n",
      "src.model - INFO - Test f1-score: 0.9134623408317566\n",
      "52/52 [==============================] - 0s 5ms/step\n",
      "src.train - INFO - Confusion Matrix for classes ['a', 'am', 'bm', 'c', 'd', 'dm', 'e', 'em', 'f', 'g']:\n",
      "[[146   8   2   1   0   0   1   0   0   0]\n",
      " [  2 168   3   0   0   0   0   0   1   0]\n",
      " [  1   1 141   3   0   0   0   2   0   0]\n",
      " [  2   7   0 134   1   1   2   1   3   3]\n",
      " [  0   1   2   0 129  22   0   1   0   1]\n",
      " [  0   0   1   2   8 156   2   5   2   4]\n",
      " [  2   0   1   2   1   1 152   8   1   0]\n",
      " [  1   0   0   4   1   2   1 162   1   3]\n",
      " [  0   2   0   0   0   2   0   0 136   3]\n",
      " [  2   0   1   1   1   5   2   1   1 175]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import keras\n",
    "import os, glob\n",
    "import logging\n",
    "import librosa, librosa.display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "from src.metrics import *\n",
    "from settings import *\n",
    "from src.data import generate\n",
    "from src.processing import *\n",
    "from src.model import CNN\n",
    "from src.data.preprocessing import get_most_shape\n",
    "from setup_logging import setup_logging\n",
    "\n",
    "setup_logging()\n",
    "logger = logging.getLogger('src.train')\n",
    "\n",
    "train_datas = []\n",
    "test_datas = []\n",
    "\n",
    "for dataset in datasets_raw:\n",
    "    train_data, test_data = train_test_split(dataset, augmented=False, split_ratio=0.65)\n",
    "    train_datas.append(train_data)\n",
    "    test_datas.append(test_data)\n",
    "\n",
    "for dataset in datasets_augmented:\n",
    "    train_data, test_data = train_test_split(dataset, augmented=False, split_ratio=0.65)\n",
    "    train_datas.append(train_data)\n",
    "    test_datas.append(test_data)\n",
    "\n",
    "train_data = pd.concat(train_datas)\n",
    "test_data = pd.concat(test_datas)\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "most_shape = get_most_shape(train_data)\n",
    "\n",
    "logger.info(f\"Number of train samples: {len(train_data)}\")\n",
    "logger.info(f\"Number of test samples: {len(test_data)}\")\n",
    "# most_shape = get_most_shape(dataset)\n",
    "#train_data, test_data = train_test_split(dataset, augmented=augmented, split_ratio=0.65)\n",
    "\n",
    "X_train, y_train = features_target_split(train_data)\n",
    "X_test, y_test = features_target_split(test_data)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train, X_test = reshape_feature_CNN(X_train, size=max_spectrogram_size), reshape_feature_CNN(X_test, size=max_spectrogram_size)\n",
    "\n",
    "# Preserve y_test values\n",
    "y_test_values = y_test.copy()\n",
    "\n",
    "# One-Hot encoding for classes\n",
    "y_train, y_test = one_hot_encode(y_train), one_hot_encode(y_test)\n",
    "\n",
    "# Instance of CNN model\n",
    "cnn = CNN(most_shape)\n",
    "logger.info(str(cnn))\n",
    "\n",
    "cnn.train(X_train, y_train, X_test, y_test, batch_size=16)\n",
    "cnn.evaluate(X_train, y_train, X_test, y_test)\n",
    "\n",
    "if tf.__version__ != '1.8.0':\n",
    "    predict_x=cnn.model.predict(X_test)\n",
    "    predictions = np.argmax(predict_x,axis=1)\n",
    "else:    \n",
    "    predictions = cnn.model.predict_classes(X_test)\n",
    "conf_matrix=confusion_matrix(y_test_values, predictions, labels=range(10))\n",
    "logger.info('Confusion Matrix for classes {}:\\n{}'.format(CLASSES, conf_matrix))\n",
    "#cnn.save_model(name=\"model_all_data_augment_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4463 - accuracy: 0.8957 - precision: 0.9109 - recall: 0.8825 - fmeasure: 0.8961\n",
      "Test score for instrument: Guitar\n",
      "\tTest loss: 0.44629520177841187\n",
      "\tTest accuracy: 0.895714282989502\n",
      "\tTest precision: 0.910923421382904\n",
      "\tTest recall: 0.8825080990791321\n",
      "\tTest f1-score: 0.8961212038993835\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6718 - accuracy: 0.9619 - precision: 0.9706 - recall: 0.9594 - fmeasure: 0.9649\n",
      "Test score for instrument: Accordion\n",
      "\tTest loss: 0.6718100905418396\n",
      "\tTest accuracy: 0.961904764175415\n",
      "\tTest precision: 0.9706451296806335\n",
      "\tTest recall: 0.9593750238418579\n",
      "\tTest f1-score: 0.9648745656013489\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1668 - accuracy: 0.9492 - precision: 0.9599 - recall: 0.9438 - fmeasure: 0.9514\n",
      "Test score for instrument: Violin\n",
      "\tTest loss: 0.16679321229457855\n",
      "\tTest accuracy: 0.9492063522338867\n",
      "\tTest precision: 0.9599078893661499\n",
      "\tTest recall: 0.9437500238418579\n",
      "\tTest f1-score: 0.9514285326004028\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5634 - accuracy: 0.8571 - precision: 0.8985 - recall: 0.8307 - fmeasure: 0.8613\n",
      "Test score for instrument: Piano\n",
      "\tTest loss: 0.5633774399757385\n",
      "\tTest accuracy: 0.8571428656578064\n",
      "\tTest precision: 0.898516833782196\n",
      "\tTest recall: 0.8306713104248047\n",
      "\tTest f1-score: 0.8612680435180664\n"
     ]
    }
   ],
   "source": [
    "test_instruments = instruments + instruments_aug\n",
    "for test_data, instrument in zip(test_datas, test_instruments):\n",
    "    X_test = test_data['spectrogram']\n",
    "    X_test = np.array([x.reshape( (128, max_spectrogram_size, 1) ) for x in X_test])\n",
    "    y_test = test_data['class_ID']\n",
    "\n",
    "    y_test_values=y_test\n",
    "    y_test = np.array(keras.utils.to_categorical(y_test, 10))\n",
    "\n",
    "    score = cnn.model.evaluate(X_test,y_test)\n",
    "    print(f'Test score for instrument: {instrument}')\n",
    "    print('\\tTest loss:', score[0])\n",
    "    print('\\tTest accuracy:', score[1])\n",
    "    print('\\tTest precision:', score[2])\n",
    "    print('\\tTest recall:', score[3])\n",
    "    print('\\tTest f1-score:', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.model - INFO - Saving model\n",
      "src.model - INFO - Saved model to /home/tzag/danigil/dl/guitarCR/models\n"
     ]
    }
   ],
   "source": [
    "cnn.save_model(name=\"model_alldata_augment_batchsize16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 128, 87, 1), found shape=(None, 128, 107, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m y_test_values\u001b[39m=\u001b[39my_test\n\u001b[1;32m     18\u001b[0m y_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(y_test, \u001b[39m10\u001b[39m))\n\u001b[0;32m---> 20\u001b[0m score \u001b[39m=\u001b[39m baseline_model\u001b[39m.\u001b[39;49mevaluate(X_test,y_test)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTest score for instrument: \u001b[39m\u001b[39m{\u001b[39;00minstrument\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[39m# print('\\tTest loss:', score[0])\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# print('\\tTest accuracy:', score[1])\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# print('\\tTest precision:', score[2])\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# print('\\tTest recall:', score[3])\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filekcblrgyb.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/tzag/miniconda3/envs/danigil-steganalysis/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 128, 87, 1), found shape=(None, 128, 107, 1)\n"
     ]
    }
   ],
   "source": [
    "with open(MODEL_1_JSON, \"r\") as json_file:\n",
    "\tloaded_model_json = json_file.read()\n",
    "\n",
    "baseline_model = model_from_json(loaded_model_json)\n",
    "baseline_model.load_weights(MODEL_1_H5)\n",
    "\n",
    "baseline_model.compile(\n",
    "            optimizer=\"Adam\",\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=['accuracy', precision, recall, fmeasure])\n",
    "\n",
    "for test_data, instrument in zip(test_datas, test_instruments):\n",
    "    X_test = test_data['spectrogram']\n",
    "    X_test = np.array([x.reshape( (128, max_spectrogram_size, 1) ) for x in X_test])\n",
    "    y_test = test_data['class_ID']\n",
    "\n",
    "    y_test_values=y_test\n",
    "    y_test = np.array(keras.utils.to_categorical(y_test, 10))\n",
    "\n",
    "    score = baseline_model.evaluate(X_test,y_test)\n",
    "    print(f'Test score for instrument: {instrument}')\n",
    "    # print('\\tTest loss:', score[0])\n",
    "    # print('\\tTest accuracy:', score[1])\n",
    "    # print('\\tTest precision:', score[2])\n",
    "    # print('\\tTest recall:', score[3])\n",
    "    print('\\tf1-score:', score[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danigil-steganalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
